<html>

<head>
<meta http-equiv=Content-Type content="text/html; charset=windows-1252">
<meta name=Generator content="Microsoft Word 15 (filtered)">
<style>
<!--
 /* Font Definitions */
 @font-face
	{font-family:Wingdings;
	panose-1:5 0 0 0 0 0 0 0 0 0;}
@font-face
	{font-family:"Cambria Math";
	panose-1:2 4 5 3 5 4 6 3 2 4;}
@font-face
	{font-family:Calibri;
	panose-1:2 15 5 2 2 2 4 3 2 4;}
 /* Style Definitions */
 p.MsoNormal, li.MsoNormal, div.MsoNormal
	{margin-top:0cm;
	margin-right:0cm;
	margin-bottom:8.0pt;
	margin-left:0cm;
	line-height:107%;
	font-size:11.0pt;
	font-family:"Calibri",sans-serif;}
p.MsoFooter, li.MsoFooter, div.MsoFooter
	{mso-style-link:"Footer Char";
	margin:0cm;
	font-size:11.0pt;
	font-family:"Calibri",sans-serif;}
p.MsoCaption, li.MsoCaption, div.MsoCaption
	{margin-top:0cm;
	margin-right:0cm;
	margin-bottom:10.0pt;
	margin-left:0cm;
	font-size:9.0pt;
	font-family:"Calibri",sans-serif;
	color:#44546A;
	font-style:italic;}
a:link, span.MsoHyperlink
	{color:#0563C1;
	text-decoration:underline;}
a:visited, span.MsoHyperlinkFollowed
	{color:#954F72;
	text-decoration:underline;}
p.MsoListParagraph, li.MsoListParagraph, div.MsoListParagraph
	{margin-top:0cm;
	margin-right:0cm;
	margin-bottom:8.0pt;
	margin-left:36.0pt;
	line-height:107%;
	font-size:11.0pt;
	font-family:"Calibri",sans-serif;}
p.MsoListParagraphCxSpFirst, li.MsoListParagraphCxSpFirst, div.MsoListParagraphCxSpFirst
	{margin-top:0cm;
	margin-right:0cm;
	margin-bottom:0cm;
	margin-left:36.0pt;
	line-height:107%;
	font-size:11.0pt;
	font-family:"Calibri",sans-serif;}
p.MsoListParagraphCxSpMiddle, li.MsoListParagraphCxSpMiddle, div.MsoListParagraphCxSpMiddle
	{margin-top:0cm;
	margin-right:0cm;
	margin-bottom:0cm;
	margin-left:36.0pt;
	line-height:107%;
	font-size:11.0pt;
	font-family:"Calibri",sans-serif;}
p.MsoListParagraphCxSpLast, li.MsoListParagraphCxSpLast, div.MsoListParagraphCxSpLast
	{margin-top:0cm;
	margin-right:0cm;
	margin-bottom:8.0pt;
	margin-left:36.0pt;
	line-height:107%;
	font-size:11.0pt;
	font-family:"Calibri",sans-serif;}
span.FooterChar
	{mso-style-name:"Footer Char";
	mso-style-link:Footer;}
.MsoChpDefault
	{font-family:"Calibri",sans-serif;}
.MsoPapDefault
	{margin-bottom:8.0pt;
	line-height:107%;}
 /* Page Definitions */
 @page WordSection1
	{size:595.3pt 841.9pt;
	margin:72.0pt 72.0pt 72.0pt 72.0pt;}
div.WordSection1
	{page:WordSection1;}
@page WordSection2
	{size:595.3pt 841.9pt;
	margin:72.0pt 72.0pt 72.0pt 72.0pt;}
div.WordSection2
	{page:WordSection2;}
 /* List Definitions */
 ol
	{margin-bottom:0cm;}
ul
	{margin-bottom:0cm;}
-->
</style>

</head>

<body lang=EN-GB link="#0563C1" vlink="#954F72" style='word-wrap:break-word'>

<div class=WordSection1>

<p class=MsoNormal style='line-height:150%'><b><span style='font-size:16.0pt;
line-height:150%;font-family:"Times New Roman",serif'>SPEECH EMOTION
CLASSIFICATION USING SPECTROGRAMS AND IMAGE CLASSIFICATION IN TENSORFLOW</span></b></p>

<p class=MsoNormal style='line-height:150%'><b><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Abstract</span></b></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>A
variety of approaches are currently used in the field of <a name="_Hlk123856367">Speech
Emotion Recognition (SER), </a>with the main goal of recognising salient
discriminants and valuable characteristics of speech signals. These traits are
classified to detect a speaker's distinct emotions. Deep learning techniques
have recently emerged as a breakthrough in voice emotion detection, allowing
for the identification and classification of emotions. </span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>We
used the RAVDESS dataset in this project, which consists of audio recordings of
actors speaking in various emotional states. In this project, we used
TensorFlow to train a convolutional neural network (CNN) to classify emotions
in speech audio. We used image classification techniques to train a
convolutional neural network (CNN) to classify the emotions after
pre-processing the audio by converting it into spectrogram images. To improve
the model's generalisation ability, we used Spec Augment on the spectrogram
images during training, which involved time and frequency masking.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>To
assess the model's performance, we divided the training data into 80% for
training and 20% for validation and tested the model by varying parameters such
as epochs, learning rate, batch size, and optimizer. Despite these efforts, the
model's validation accuracy was only about 56%. This suggests that the approach
of using spectrogram images and image classification techniques may not be
effective for accurately classifying emotions in speech.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><b><i><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Keywords:
Speech Emotion Recognition (SER), </span></i></b><b><i><span style='font-size:
14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Convolutional
Neural Network (CNN), RAVDESS dataset, Image classification, spectrograms,
TensorFlow</span></i></b></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><b><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>&nbsp;</span></b></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><b><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>1
- Introduction </span></b></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>This
chapter introduces the background of this research project, as well as related
objectives and benefits, and provides an overview of the content of subsequent
chapters.</span></p>

<p class=MsoNormal style='margin-bottom:0cm;margin-bottom:0cm;margin-top:0cm;
text-align:justify;line-height:150%'><a name="_Hlk123861592"><b><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>1.1-
Background</span></b></a></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Human
emotions are extremely difficult to comprehend mathematically. Facial
expressions are one of the most accurate ways to predict a person's emotional
state. Speech is another modality that can be used. Speech is a complex signal containing
information about the message, the speaker, the language, and the speaker's
emotions. There are various types of emotions that can be expressed through
speech. Emotional speech recognition is a system that simply recognises a human
being's emotional state from his or her voice; speech can be deceptive even for
humans to determine the speaker's sentiment (Lingli Yu, 2013).</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>The
most natural and fastest way for humans and computers to exchange information
and communicate is through speech-emotion recognition (SER), which is critical
in real-time applications of human-machine interaction. Speech signals
generated by SER sensors are an active area of research in digital signal
processing. They are used to recognise the qualitative emotional state of
speakers using speech signals, which contain more information than spoken
words. SER has a variety of applications, including audio surveys, virtual
assistants in call centres, lie detection, and video games. It is also useful
in therapeutic applications such as mood monitoring, mobile health care, and
the diagnosis of bipolar illness (Gideon and&nbsp;Provost 2016).</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>The
human brain has a powerful information processing ability that allows it to
recognise and interpret sections of a speech where the audio reflects the
speaker's mood; as a result, it can detect changes in the speaker's sentiment
in address. Speech includes both textual and auditory data because it is the
most direct and effective way for people to communicate and exchange
information on a daily basis (Yadav,2021). Auditory information refers to
aspects of speech that the human ear can detect, such as timbre, tone quality,
loudness, and rhythm. Because the acoustic properties of different emotional
addresses differ noticeably, mathematical computation methods are frequently
used to translate the speech signal into the components associated with its
emotion (Agarwal and Om,2021).</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>The
computer may recognise a speech for the mood represented. Until now, the
majority of research on voice emotion recognition has relied on two approaches:
classical machine learning and deep learning. Commonly used machine learning
methods include the Hidden Markov model (HMM), K proximity algorithm, support
vector machine (SVM), and Bayesian algorithm; as machine learning algorithms
mature, how to make machines think like human brains and provide behavioural
feedback becomes a focus of more and more scholars' research, which is also the
point of deep learning research. Deep learning simulates how the human brain
processes information by building multilayer neural networks, integrating
feature representation with knowledge, and developing a model through
continuous learning. Convolutional neural networks (CNN) and recurrent neural
networks (RNN) are popular deep learning methods for spatial data and time
series data, respectively (Shen et al., 2022).</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>A
spectrogram is a visual representation of a signal's spectral content over
time, such as an audio signal. Applying a Fourier transform to a windowed
segment of the signal and plotting the resulting complex spectrum as a function
of time can be used to calculate it. The spectral content of a speech signal
can contain information about the speaker's emotion, so spectrograms have been
used as a feature for speech emotion recognition. Certain frequency bands or
patterns in the spectrogram, for example, may be indicative of specific
emotions, such as high pitch or intensity for anger or sadness, or low pitch or
intensity for happiness or calmness(Liu et al. (2011).</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Deep
learning approaches, such as convolutional neural networks (CNNs) and recurrent
neural networks (RNNs), have also been used for emotion recognition in speech
signals, with spectrogram as input. The spectrogram is typically treated as an
image in these approaches and fed into the CNN or RNN, which learns to extract
relevant features and classify the emotion based on the spectrogram (Trigeorgis
et al,. 2016).</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>This
project aims to classify the speech emotion spectrogram using the various
algorithms and approaches that are for image classification using TensorFlow.</span></p>

<p class=MsoListParagraph style='margin-left:18.0pt;text-align:justify;
text-indent:-18.0pt;line-height:150%'><b><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>1.2<span
style='font:7.0pt "Times New Roman"'> </span></span></b><b><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>-<a
name="_Hlk123861616"> Objectives</a></span></b></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>The
task of recognising the emotional content of speech is known as speech emotion
detection. It is a significant issue in the field of affective computing, with
potential applications in human-computer interaction, education, and
healthcare. This project aims to create a system for detecting speech emotions
using spectrograms and image classification techniques in TensorFlow.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Spectrograms
are visual representations of the frequency spectrum of sound that can reveal information
about the emotional content of speech. We intend to use TensorFlow to train an
image classification model to classify the spectrograms after converting audio
data into spectrograms.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>This
research aimed to address the model's performance on the RAVDESS dataset and find
its accuracy. We will also look into how different machine learning models can
be sensitive to the choice of hyperparameters, such as the learning rate, batch
size etc and preprocessing techniques that affect system performance. In addition,
we will investigate the system's potential applications and identify any
challenges or limitations of the approach. The overall goal of this project is
to use TensorFlow to create a robust and accurate system for speech emotion
detection.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Measurable
Objectives include:</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Creating
effective spectrograms from the datasets.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>To
build a model that can classify spectrograms into different emotions using an image
classifier </span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>To
investigate the impact of training parameters on the performance of the system</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Validation
using performance measures.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Emotion
Prediction using feature extraction and classification.</span></p>

<p class=MsoListParagraph style='margin-left:18.0pt;text-align:justify;
text-indent:-18.0pt;line-height:150%'><b><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>1.3<span
style='font:7.0pt "Times New Roman"'> </span></span></b><b><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>-
Research Benefits</span></b></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Deep
learning techniques, which are effective at recognising patterns in data, may
be used to achieve high accuracy in speech emotion detection. TensorFlow
provides efficient tools for training and deploying machine learning models,
which can be useful when developing a system capable of classifying large
amounts of data promptly. Many potential applications exist for the ability to
recognise the emotional content of speech, including human-computer
interaction, education, and healthcare. Improved human-machine interaction:
Recognizing the emotional content of speech may enable the development of
systems that can better understand and respond to human emotions, resulting in
more natural and effective human-machine interactions.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Potential
for automation: It may be possible to free up time and resources that would
otherwise be spent on manual annotation and analysis by automating the process
of recognising emotions in speech.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Increased
understanding of human emotions: It may be possible to gain a deeper
understanding of how emotions are expressed through language by studying the
patterns in the speech that correspond to different emotions.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal style='margin-bottom:0cm;margin-bottom:0cm;margin-top:0cm;
text-align:justify;line-height:150%'><a name="_Hlk123861758"><b><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>2-
Literature Review</span></b></a></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>In
this section, we discuss the relevant literature on the Overview of Speech Emotion
Classification, Previous Work on Speech Emotion Classification, and Previous
Successful Application of Tensor Flow in Image Classification</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><a
name="_Hlk123861822"><b><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'>2.1 - Overview of speech emotion
classification</span></b></a></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>For
more than two decades (Schuller, 2018), SER has been used in human-computer
interaction (Cowie et al., 2001), robots (Huahu et al., 2010), mobile services
(Yoon et al., 2007), call centres (Gupta and Rajput, 2007), computer games
(Szwoch and Szwoch, 2015), and psychological assessment (Lancker et al., 1989; Low
et al., 2011). Despite its many applications, emotion detection is a difficult
task because emotions are subjective. There is no universal agreement on how to
measure or categorise them. They are judged based on their perceptions of other
humans, and even we have been known to misinterpret them at times.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>An
SER system can be defined as a collection of methodologies for processing and
classifying speech signals to detect emotions embedded in them. We can divide
it into several categories, as shown in Fig. 1. A SER system necessitates the
following: a classifier, a supervised learning construct that will be trained
to recognise emotions in new speech signals. </span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=602 height=298 id="Picture 1"
src="Speech%20Emotion%20Classification.files/image001.png"
alt="Diagram&#10;&#10;Description automatically generated"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><a name="_Hlk123863840"><i><span style='font-size:14.0pt;
font-family:"Times New Roman",serif;color:#44546A'>Figure 2.1: An overview of
speech emotion recognition systems. The recognition requirements flow from left
to right (Akcaya and O&#287;uz, 2019)</span></i></a></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Such
a supervised system provides the necessity of labelled data that have emotions
embedded in them. The data must be preprocessed before its features can be
extracted. A classification process relies on features. They distil the
original data down to its most important characteristics. Signals can be used
in speech and can be classified into four categories: prosodic, spectral, voice
quality, and Teager energy operator features. Depending on the application and
availability, the classifier can be strengthened by incorporating additional
features from other modalities, such as visual or linguistic. All of these
features are then passed to the classification system, which has a large number
of classifiers at its disposal. Classifiers that incorporate deep learning have
recently become common. (Akcaya and O&#287;uz,2019)</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><b><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>2.1.1
- Emotions </span></b></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>There
is no agreement about the definition of emotion, and it remains an open issue
in psychology. Plutchik claims that more than ninety definitions of emotion
were proposed in the twentieth century (Plutchik, 2001). Emotions are complex
psychological states made up of various components such as personal experience,
physiological, behavioural, and communicative reactions. Based on these
definitions, two models for speech emotion recognition have emerged: the
discrete emotional model and the dimensional emotional model.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Discrete
emotion theory is based on Ekman and Oster's (1979); Ekman et al. (2013) six
categories of basic emotions: sadness, happiness, fear, anger, disgust, and
surprise. These inborn and culturally independent emotions last only a short
time (Ekman, 1971). Other emotions are created by combining the basic ones. The
majority of existing SER systems concentrate on these fundamental emotional
categories.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>An
alternative model that uses a small number of latent dimensions to characterise
emotions such as valence, arousal, control, and power is the Dimensional
emotional model (Russell and Mehrabian, 1977; Watson et al., 1988). These
dimensions are absolute and universal aspects of emotion. Emotions are not
independent of one another in the dimensional approach; rather, they are
analogous to one another in a systematic way. A two-dimensional model that uses
arousal, activation, or excitation on one dimension and valence, appraisal, or
evaluation on the other is one of the most preferred dimensional models. The
valence dimension, which ranges from unpleasant to pleasant, describes whether
an emotion is positive or negative. The arousal dimension characterises the
intensity of the felt emotion. It can range from boredom to frantic excitement
and can be excited or apathetic (Nicolaou et al., 2011). The three-dimensional
model includes a dimension of dominance or power, which refers to the person's
apparent strength between weak and strong. For example, the third dimension
distinguishes between anger and fear by taking into account the person's
strengths or weaknesses (Grimm et al., 2007).</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><b><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>2.1.2
- Databases </span></b></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Databases
for speech emotion recognition can be investigated in three parts: Acted
(Simulated) speech emotion databases, Elicited (Induced) speech emotion
databases, and Natural speech emotion databases.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Professional
or semi-professional actors in sound-proof studios record utterances in acted
speech databases. It is easier to create such a database than the other
methods; however, the researchers claim that acted speech cannot adequately
convey real-life emotions and may even be exaggerated. This reduces the rate of
recognition of real-life emotions.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Elicited
speech databases are built by immersing speakers in a simulated emotional
situation that can elicit a variety of emotions. Although the emotions are not
fully elicited, they are realistic.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Natural
speech databases are primarily sourced from talk shows, call centre recordings,
radio interviews, and other similar sources. These real-world speeches are sometimes
referred to as spontaneous speech. It is more difficult to obtain data because
ethical and legal issues arise when processing and distributing them. (Akcaya and
O&#287;uz,2019)</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><b><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>2.2
- Previous work on speech emotion classification based on features</span></b></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Speech
is a variable-length continuous signal that conveys both information and
emotion. As a result, depending on the approach used, global or local features
can be extracted. The gross statistics such as mean, minimum and maximum
values, and standard deviation are represented by global features, also known
as long-term or supra-segmental features. Local features, also known as
short-term or segmental features, represent temporal dynamics to approximate a
stationary state. These stationary states are significant because emotional
characteristics are not evenly distributed across all positions of the speech
signal (Rao et al., 2013).</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>These
SER systems local and global characteristics are classified into four
categories.: Prosodic Characteristics, Spectral Characteristics, Voice Quality Characteristics
and Teager Energy Operator (TEO) Based Characteristics</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>In
SER systems, prosodic and spectral features are more commonly used. Depending
on the study's approach, some of the features are classified into different
categories. TEO features are specifically intended to detect stress and anger.
These characteristics are described individually; however, in practice, they
are frequently combined to achieve better results. (Zeng et al., 2009)</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><b><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>2.2.1
- Prosodic features</span></b></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>The
most common prosodic features are fundamental frequency, energy, and duration.
The vibrations in the vocal cord produce the fundamental frequency, F0. It
produces the rhythmic and tonal characteristics of speech. The change in
fundamental frequency over the course of an utterance produces its fundamental
frequency contour, which can be used as a feature. The energy of the speech
signal, also known as volume or intensity, provides a representation of the
amplitude variation of speech signals over time. According to researchers,
high-arousal emotions such as anger, happiness, or surprise result in increased
energy, whereas disgust and sadness result in decreased energy (Lin et al.,
2012).</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>There
are links between prosodic characteristics and emotional states. Prosodic
features reveal the changes that occur during an emotional speech. For example,
mean F0, F0 variability, and vocal intensity increase during the production of
high-level arousal emotions such as anger, fear, anxiety, and joy. During the
expression of anger, the F0 contour decreases over time. It, on the other hand,
increases over time during the expression of joy. When compared to natural
speech, low-level arousal, such as sadness, results in lower mean F0, F0
variability, and vocal intensity, while F0 also decreases over time (Frick,
1985; Bachorowski, 1999).</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>The
time it takes to express anger is less than the time it takes to express
sadness (Rao et al., 2013). Many studies have been conducted on various aspects
of prosodic features. Frick (1985); Cowie and Douglas-Cowie (1985) investigate
prosodic features and their relationship to emotional states (1996). When
prosodic features are used, some studies show that SER systems achieve similar
or better results than human judges (Nogueiras et al., 2001; Luengo et al.,
2005).</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Several
studies look at the performance of prosodic features based on their
granularity. Schuller et al. use a continuous Hidden Markov Model to compare
gross statistics of pitch and energy contours to instantaneous pitch and energy
features (Schuller et al., 2003). They obtained a recognition rate of 86.6%
using global features, 77.6% using local features, and 79.8% using human
judges. Rao et al. compared local and global prosodic features as well as their
combination (Rao et al., 2013).</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><b><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>2.2.2
- Spectral features</span></b></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>When
a person produces sound, the shape of the vocal tract filters it. This shape
determines the sound that is produced. An accurate representation of the vocal
tract and the sound produced may result from an accurately simulated shape. The
frequency domain accurately represents vocal tract characteristics (Koolagudi
and Rao, 2012).</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Linear
Prediction Cepstral Coefficients (LPCC) also represent speaker vocal tract
characteristics. These characteristics differ in relation to specific emotions.
Linear Prediction Coefficient can be directly obtained using a recursive method
(LPC). LPC is essentially the coefficient of all-pole filters and is equivalent
to the smoothed envelope of the speech log spectrum (Wong and Sridharan, 2001).</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Another
feature, Log-Frequency Power Coefficients (LFPC), simulates the human auditory
system's logarithmic filtering characteristics by measuring spectral band
energies with the Fast Fourier Transform (Nwe et al., 2003).</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Sato
et al. employ segmental MFCC features for emotion recognition in speech (Sato
and Obuchi, 2007). They used multi-template MFCC clustering to label each
frame. They compared the performance of prosody-based algorithms with k-nearest
neighbours to conventional MFCC-based algorithms with HMM. They improved their
performance by using the new method.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Bitouk
et al. proposed a new set of spectral features that are MFCC statistics
calculated across three phoneme-type classes of interest-stressed and
unstressed vowels and consonants in the utterance (Bitouk et al., 2010). When
compared to prosodic features or utterance-level spectral features, the
proposed features produced more accurate results. Furthermore, combining these
features with prosodic features improves accuracy. It has also been discovered
that the consonant regions of the utterance contain more emotional information
than the stressed and unstressed vowel features.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><b><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>2.3
- <a name="_Hlk123861988">Previous work on speech emotion classification based
on Classifiers</a></span></b></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Underlying
emotions for a given utterance are classified by speech emotion recognition
systems. Many machine learning algorithms, including traditional classifiers
and deep learning algorithms, are used to perform the speech emotion
recognition task. However, as with any complex problem, there is no widely
accepted machine learning algorithm that can be used; current studies are
mostly empirical.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><b><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>2.3.1
- Traditional classifiers</span></b></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Hidden
Markov Model (HMM), Gaussian Mixture Model (GMM), Support Vector Machines (SVM),
and Artificial Neural Networks are the most popular algorithms (ANN).
Classification methods based on Decision Trees (DT), k-Nearest Neighbour
(k-NN), k-means, and Naive Bayes Classifiers are also available. In addition to
using single classifiers, ensemble methods for SER are used to combine several
classifiers to achieve better results.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>A,The
Markov Chain Model. The Hidden Markov Model is a popular method for speech
recognition that has been successfully extended to recognise emotions.
Nogueiras et al. used hidden semi-continuous Markov models to compute the
contours of low-level pitch and energy features (Nogueiras et al., 2001). They
achieved a recognition rate of more than 70% for six emotion classes:
happiness, anger, joy, fear, disgust, and sadness. Nwe et al. demonstrated that
the LFPC feature on the Hidden Markov Model (HMM) outperforms the MFCC and LPCC
(Nwe et al., 2003a). They achieved a recognition rate of 77.1% and 89% for
average and best recognition rates, respectively, while human recognition was
65.8%.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Lin
et al. classified five emotions using HMM and SVM: anger, happiness, sadness,
surprise, and neutrality (Lin and Wei, 2005). For HMM, 39 candidate features
are extracted, and then SFS is used to select features. The performance of classification
with selected features is compared to classification using MFCC. A new vector
is constructed from the difference in Mel frequency scale sub-band energies,
and the performance of the K-nearest neighbour is tested using this newly
constructed vector. For the speaker-dependent case, HMM achieves 99.5%
accuracy, while SVM achieves 88.9% accuracy.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>B,
Artificial Neural Networks: Artificial Neural Networks are a popular method for
a variety of classification problems. It is made up of an input layer, one or
more hidden layers, and an output layer.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Nicholson
et al. developed a speaker and context-independent SER system using ANN
(Nicholson et al., 2000). As parameters, they chose speech power, pitch, LPC,
and delta LPC. They constructed a sub-neural network for each emotion in the
database. Each neural network produces a value that represents the probability
that the utterance corresponds to an emotion. The decision logic selects the
best prediction based on this value. Using this one-class-in-one neural
network, they achieved a 50% average recognition rate.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Petrushin
and colleagues created a call-center application (Petrushin, 1999). They put
their system through its paces with ANN and an ensemble of ANN and k-nearest
neighbour (kNN) classifiers. As a feature set, they chose to pitch, energy,
speaking rate, and first and second formants statistics. Average accuracies of
55%, 65%, and 70% are obtained for kNN, ANN, and the ensemble of ANNs,
respectively.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><b><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>2.3.2
- Classifiers based on deep learning</span></b></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Deep
neural networks are commonly used to refer to deep learning algorithms that are
based on Artificial Neural Networks. The term &quot;deep&quot; refers to the
number of hidden layers, which can reach hundreds, as opposed to a traditional
neural network, which has two or three hidden layers.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>A,
Recurrent Neural Networks (RNNs). RNNs are a type of neural network that is
trained to process sequential data. They can remember the received input data
and make a precise prediction about what will happen next by using internal memory.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Eyben
et al. proposed an LSTM-RNN-based online SER system (Eyben et al., 2010). They
created a new 3D model by adding the time dimension to the 2D activation
valence emotional model.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Wollmer
et al. (2008) used LSTM-RNN for continuous emotion recognition in a 3D space
spanned by activation, valence, and time. Belfast Sensitive Artificial Listener
data from the HUMAINE database is used for experiments. LSTM-RNN produces the
best activation results (MSE 0.08). However, for the valence dimension,
LSTM-RNN and SVR performed equally (MSE 0.18). They also discovered that
valence classification performance was relatively low, as detecting valence
from only acoustic features is known to be difficult. As a result, they
advocate for the use of additional modalities, such as linguistic features for
valence classification. Other studies have found that semantic features improve
valence estimation while emotional features improve arousal elicitation.(Karado&#287;an
and Larsen, 2012; Asgari et al., 2014).</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>B,
Convolutional Neural Networks: Convolutional Neural Networks (CNNs) are
specialised neural networks that are used to process data with a grid-like
topology, such as images.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Trigeorgis
et al. proposed a system that combines CNN and LSTM networks, with CNN used to
learn the best descriptive characteristics of the speech signal directly from
the raw time representation (Trigeorgis et al., 2016). After segmenting the
speech signals, denoising is applied as a preprocessing step. Acoustic features
are then extracted using CNN. Finally, the extracted features are fed into an
LSTM layered deep RNN. When compared to traditionally designed features, the
proposed method achieves significantly better performance. Lim et al. take a
similar approach (2016). They compared CNN and LSTM-RNN to the proposed time-distributed
CNN. The proposed method yielded an average precision of 88.01%, while a single
CNN classifier and an LSTM classifier yielded 86.32% and 78.31%, respectively.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>CNNs
can be built in a variety of dimensions. Zhao et al. used 1-D and 2-D CNNs with
an LSTM network to recognise speech emotions (Zhao et al., 2019). 1-D CNNs are
designed to learn local and global emotion-related features from speech,
whereas 2-D CNNs are used to learn Mel spectrograms. The 2-D network
outperformed the 1-D network. On Berlin EmoDB, 2-D networks achieved
recognition rates of 95.33% and 95.89% with speaker-dependent and
speaker-independent experiments, respectively, whereas on the IEMOCAP database,
recognition rates are 89.16% and 52.14% for speaker-dependent and speaker-independent
experiments, respectively.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>While
CNN is used in the majority of SER systems, at least one LSTM network is added
to deal with temporal dependencies and spectral variations. It does, however,
increase the depth and complexity. Kim et al. instead proposed a 3-D CNN to
learn spectro-temporal features (Kim et al., 2017b). After extracting two
seconds of segments from utterances, padding is applied, and 256-point spectrograms
are extracted every 20 ms. There are a total of 100 frames obtained. Finally, a
temporal series of 10-by-256-pixel 2-D feature maps are created. Spectral
features are represented as feature maps in 200 ms long short-term windows.
Each utterance segment has a resolution of 10x10x256 and is divided into three
categories: short-term (T), long-term (L), and spectral (S). These feature maps
are fed into 3-D CNN, which employs 3-D max polling. Then, two learning methods
are tested. To begin, 3-D output features are flattened. They are converted
into 1-D vectors and forwarded to fully-connected layers with an additional SoftMax
layer. This technique is known as 3D-CNN-DNN. The second method converts the
3-D output into 2-D output features, which are then forwarded into a temporal
series of a fully connected layer. Both proposed methods outperformed
commercially available methods.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><b><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>2.4
- <a name="_Hlk123862166">Literature review on image classification using
TensorFlow</a></span></b></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Abadi
et al. (2015): &quot;TensorFlow: A system for large-scale machine
learning.&quot; TensorFlow, a system for large-scale machine learning that is
flexible, efficient, and simple to use, is presented in this paper. TensorFlow
is built on dataflow graphs, in which computations are represented as nodes and
data as edges. It also includes a library of machine learning algorithms for
tasks like image classification, and it has been used to train large-scale deep
learning models on a variety of datasets, including the ImageNet dataset.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>&quot;Deep
learning for image classification: A review&quot; (Zhang and Chen, 2018): This
review paper gives an overview of the various deep learning architectures and
approaches used for image classification, such as convolutional neural networks
(CNNs), recurrent neural networks (RNNs), and their variants. The paper also
discusses the difficulties and limitations of deep learning for image
classification, such as the requirement for large amounts of labelled data and
the risk of overfitting. The authors also emphasise TensorFlow and other deep
learning frameworks' potential for implementing and training deep learning
models for image classification.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Jain
et al., 2019: &quot;A review on deep learning techniques applied to image
classification.&quot; This review paper examines the various deep-learning
techniques used in image classification, such as CNNs, RNNs, autoencoders, and
their variants. The paper also discusses the challenges and limitations of deep
learning for image classification, such as the need for large amounts of
labelled data and the risk of overfitting, as well as the different datasets
and evaluation metrics that have been used for image classification. TensorFlow
is also mentioned by the authors as a popular framework for developing and
training deep learning models for image classification.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><b><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>&nbsp;</span></b></p>

<p class=MsoNormal style='margin-bottom:0cm;margin-bottom:0cm;margin-top:0cm;
text-align:justify;line-height:150%'><a name="_Hlk123862207"><b><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>3-
Methodology </span></b></a></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>This
chapter contains the methodology adopted, software and tools employed and a
brief description of definitions and algorithms used in carrying out this
research. Figure 2 shows a process overview of the model. The uniqueness of the
project is the fact that this tries to predict speech emotion based using a
classification based on the spectrograms generated from the audio files.</span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><b><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=602 height=274 id="Picture 54"
src="Speech%20Emotion%20Classification.files/image002.png"></span></b></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><a name="_Hlk123863927"><i><span style='font-size:14.0pt;
font-family:"Times New Roman",serif;color:#44546A'>Figure 3.1 Methodology</span></i></a></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>The
first stage of the research is to Collect a dataset of audio samples that are
labelled with their corresponding emotions. In this task RAVDESS dataset, a pre-existing
dataset was used. Next pre-process the audio data by converting it into a
spectrogram representation. A spectrogram is a visual representation of the
spectral content of a signal, such as an audio signal. It can be calculated by
applying a Fourier transform to a windowed segment of the signal and plotting
the resulting complex spectrum as a function of time. To convert the audio data
into a spectrogram, we use the library `tfio`. The parameters for the
spectrogram, such as the window size and hop length, to control the frequency
resolution and temporal resolution of the resulting spectrogram were specified.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>The
next stage split the dataset into a training set and a validation set. The
training set will be used to train the CNN, while the test set will be used to
evaluate the model's performance. It's important to have a diverse and balanced
split of the different emotions in both the training and test sets so an 80:20
split will be utilised. After that build a convolutional neural network (CNN) in
TensorFlow to classify the spectrograms. A CNN is a type of neural network that
is well-suited for image classification tasks. When designing your own CNN, we
will need to choose the architecture, such as the number and size of the
convolutional and fully-connected layers, as well as the hyperparameters, such
as the learning rate and batch size.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Then
train the CNN on the training set and evaluate its performance on the test set.
To train the CNN, we will need to define the loss function and the optimizer,
and then use the TensorFlow fit() function to minimize the loss and update the
model parameters. We can monitor the training process using Tensor Board and
adjust the hyperparameters if needed. To evaluate the model's performance, we
can use metrics such as accuracy, precision, and recall, and compare the
results to baseline models or previous results on the same dataset. Use the
trained CNN to predict the emotions of new, unseen audio samples by feeding
their spectrograms into the model. We can use the TensorFlow predict() function
to obtain the model's predictions for the emotions of the audio samples. Then
visualize the spectrograms and the model's predictions using a library such as
matplotlib.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><b><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Understanding
the concepts used in the research</span></b></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><i><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Image
Classification (Recognition):</span></i><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'> The task of inputting an
image into a neural network and having it output some kind of label for that
image is referred to as image recognition. The network's output label will
correspond to a pre-defined class. The image can be labelled with multiple
classes or with just one. When dealing with a single class, the term
&quot;recognition&quot; is frequently used, whereas when dealing with multiple
classes, the term &quot;classification&quot; is frequently used. Object detection
is a subset of image classification in which specific instances of objects are
identified as belonging to a specific class such as animals, cars, or people (Xie
et al., 2022).</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><i><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Feature
Extraction:</span></i><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'> The neural network must extract features
in order to perform image recognition/classification. The elements of data that
you care about that will be fed through the network are referred to as
features. In image recognition, features are groups of pixels, such as edges
and points, of an object that the network will analyse for patterns. The
process of extracting relevant features from an input image for analysis is
known as feature recognition (or feature extraction). Many images include
annotations or metadata that assist the network in locating relevant features (Xie
et al., 2022).</span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=587 height=389 id="Picture 25"
src="Speech%20Emotion%20Classification.files/image003.jpg"
alt="A picture containing building material&#10;&#10;Description automatically generated"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><a name="_Hlk123863945"><i><span style='font-size:14.0pt;
font-family:"Times New Roman",serif;color:#44546A'>Figure 3.2 Convolutional
Neural Networks</span></i></a></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Feature
Extraction With Filters: The first layer of a neural network incorporates all
of the pixels in an image. After all of the data has been fed into the network,
various filters are applied to the image, resulting in representations of
various parts of the image. This is feature extraction and it creates
&quot;feature maps&quot;. A &quot;convolutional layer&quot; is used to extract
features from an image, and convolution is simply forming a representation of a
portion of an image. The term Convolutional Neural Network (CNN), the type of
neural network most commonly used in image classification/recognition, derives
from this convolution concept.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Consider
shining a flashlight over a picture in a dark room to visualise how creating
feature maps for Convolutional Networks works. we can&nbsp;learn about the
image's features as we move the beam over it. The network uses a filter to form
an image representation, and in this metaphor, the light from the flashlight is
the filter. The width of your flashlight's beam determines how much of the
image we examine at once, and neural networks have a similar parameter, filter
size. The size of the filter determines how much of the image, or how many
pixels, are examined at once. A common filter size used in CNNs is three, which
covers both height and width, examining a 3 x 3 area of pixels. All of this
computation results in a feature map. This process is typically carried out
with multiple filters, which helps to preserve the image's complexity.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=612 height=459 id="Picture 26"
src="Speech%20Emotion%20Classification.files/image004.jpg"
alt="A picture containing text, stationary&#10;&#10;Description automatically generated"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><a name="_Hlk123863964"><i><span style='font-size:14.0pt;
font-family:"Times New Roman",serif;color:#44546A'>Figure 3.3 Convolutional
Neural Networks with Feature Layers</span></i></a></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>While
the filter size specifies the filter's height and width, the filter's depth
must also be specified. Digital images are rendered as height, width, and some
RGB value that defines the colour of the pixel, so the &quot;depth&quot; being
tracked is the number of colour channels in the image. Colour images have three
depth channels, whereas grayscale images have only one. All of this means that
the dimensions of a filter of size 3 applied to a full-colour image will be 3 x
3 x 3. The network multiplies the filter values by the values in the pixels to
get a numerical representation of each pixel covered by that filter. To achieve
a complete representation, this process is repeated for the entire image. The
filter is moved across the rest of the image by a parameter called
&quot;stride,&quot; which specifies how many pixels the filter should be moved
by after calculating the value in its current position. A CNN's typical stride
length is 2.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><i><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Activation
Functions:</span></i><span style='font-size:14.0pt;line-height:150%;font-family:
"Times New Roman",serif'> After the image's feature map is created, the image's
values are passed through an activation function or activation layer. The
activation function takes image values that are in a linear form (i.e. just a
list of numbers) thanks to the convolutional layer and increases their
non-linearity because images are non-linear. A Rectified Linear Unit is a
common activation function used to accomplish this (ReLU),</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Pooling
Layers: After activation, the data is routed through a pooling layer. Pooling
&quot;downsamples&quot; an image, which means it compresses the information
that represents the image, making it smaller. The process of pooling makes the
network more adaptable and adept at recognising objects/images based on
relevant features. When we look at an image, we are usually not concerned with
all of the information in the image's background, only the features that we are
interested in, such as people or animals. Similarly, a pooling layer in a CNN
will abstract away the unnecessary parts of the image, leaving only the parts
of the image that it believes are relevant, as determined by the pooling
layer's size. Because it must make decisions about the most relevant parts of
the image, the network is expected to learn only those parts of the image that
truly represent the object in question. This helps to avoid overfitting, a
condition in which the network learns aspects of the training case too well and
fails to generalise to new data (Kurosawa, 2007).</span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=485 height=281 id="Picture 27"
src="Speech%20Emotion%20Classification.files/image005.jpg"
alt="Table&#10;&#10;Description automatically generated with low confidence"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><a name="_Hlk123863983"><i><span style='font-size:14.0pt;
font-family:"Times New Roman",serif;color:#44546A'>Figure 3.4 Max_pooling with
2x2 filter and stride = 2</span></i></a></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>There
are several methods for pooling values, but max pooling is the most commonly
used. The maximum value of the pixels within a single filter is obtained using
max pooling (within a single spot in the image). If two x two filters are used,
this reduces information by 3/4ths.The maximum values of the pixels are used to
account for possible image distortions, and the image parameters/size are
reduced to control for overfitting. Other pooling types, such as average
pooling and sum pooling, are used less frequently because max pooling produces
better accuracy (Dan Nelson, 2022).</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><i><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Flattening:</span></i><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>
The densely connected layers of our CNN require that the data be in the form of
a vector in order to be processed. As a result, the data must be
&quot;flattened&quot;. The values are compressed into a long vector or a column
of numbers in sequential order.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><i><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Fully
Connected Layer:</span></i><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'> The CNN's final layers are densely
connected layers, or an artificial neural network (ANN). The ANN's primary
function is to analyse the input features and combine them into various
attributes that will aid in classification. These layers are essentially
forming collections of neurons that represent various parts of the object in
question, such as the floppy ears of a dog or the redness of an apple. When a
sufficient number of these neurons fire in response to an input image, the
image is classified as an object.</span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=565 height=222 id="Picture 28"
src="Speech%20Emotion%20Classification.files/image006.png"
alt="A picture containing text, pool ball&#10;&#10;Description automatically generated"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><a name="_Hlk123864015"><i><span style='font-size:14.0pt;
font-family:"Times New Roman",serif;color:#44546A'>Figure 3.5 Diagram of a
multi-layer feedforward artificial neural network</span></i></a><i><span
style='font-size:14.0pt;font-family:"Times New Roman",serif;color:#44546A'>: A
Neural network with two layers</span></i></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>The
ANN computes the error or the difference between the computed values and the
expected value in the training set. The network is then subjected to
backpropagation, in which the influence of a given neuron on a neuron in the
next layer is calculated and adjusted. This is done to improve the model's
performance. This process is then repeated several times. This is how the network
learns associations between input features and output classes by training on
data. The neurons in the fully connected layers in the middle will output
binary values corresponding to the possible classes. If there are four classes
(say, a dog, a car, a house, and a person), the neuron will have a
&quot;1&quot; value for the class it believes the image represents and a
&quot;0&quot; value for the others (Dan Nelson, 2022)..</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>The
final fully connected layer will receive the output of the layer before it and
deliver a probability for each of the classes, summing to one. If there is a
0.75 value in the &quot;dog&quot; category, it represents a 75% certainty that
the image is a dog.The image classifier has now been trained, and images can be
passed into the CNN, which will now output a guess about the content of that
image (Dan Nelson, 2022).</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><b><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Workflow
for Machine Learning</span></b></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>The
process of training a neural network model is fairly standard and can be
divided into four stages.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><i><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Data
Preparation:</span></i><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'> First, gather the data and format it so
that the network can train on it. This entails gathering and labelling images.
Even if downloaded the data set is prepared by someone else, there is likely to
be some preprocessing or preparation required before we can use it for
training. Data preparation is an art in and of itself, involving issues such as
missing values, corrupted data, data in the wrong format, incorrect labels, and
so on.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><i><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Creating
the Model:</span></i><span style='font-size:14.0pt;line-height:150%;font-family:
"Times New Roman",serif'> Making decisions about various parameters and
hyperparameters is required when building the neural network model. We must
decide how many layers to use in the model, what the input and output sizes of
the layers will be, what kind of activation functions to use, whether or not to
use dropout, and so on.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><i><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Training
the Model:</span></i><span style='font-size:14.0pt;line-height:150%;font-family:
"Times New Roman",serif'> We simply create an instance of the model and fit it
to the training data after we have created the model. The length of time it
takes to train a model is the most important factor to consider. The length of
training for a network can be specified by specifying the number of epochs to
train over. The longer we train a model, the better its performance will be,
but if we train it for too long, we risk overfitting.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><i><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Model
Evaluation:</span></i><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'> The model is evaluated in several steps.
The first step in evaluating the model is to compare its performance to that of
a validation dataset, which is a data set on which the model has not been
trained. We will compare the model's performance to this validation set and
analyse it using various metrics. The most common metric for determining the
performance of a neural network model is &quot;accuracy,&quot; which is the
number of correctly classified images divided by the total number of images in
the data set.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>We
will typically go back and train the network again after seeing the accuracy of
the model's performance on a validation dataset because we are unlikely to be
satisfied with the network's performance the first time we train. We will
continue to tweak your network's parameters, retrain it, and measure its
performance until we are satisfied with the network's accuracy. Finally, we
will run a performance test on the network. This testing set contains data that
the model has never seen before. It's a good idea to keep a batch of data that
the network has never seen before for testing because all of the parameter
tweaking, combined with retesting on the validation set, could mean that the
network has learned some idiosyncrasies of the validation set that will not
generalise to out-of-sample data.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>As
a result, the testing set's purpose is to check for issues like overfitting and
increase your confidence that the&nbsp;model is truly fit to perform in the
real world.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><b><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Softwares,
tools and tecchiniques </span></b></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Python
in Google Colab was used for speech emotion classification using image
classification and spectrogram in TensorFlow. Google Colab is a free
cloud-based platform that provides access to a variety of tools and resources
for machine learning and data science, including Python and TensorFlow. It
allows you to write and run code in Python, upload and download files, and
access GPUs and TPUs for training and inference. Python is a popular
programming language for machine learning and data science, and it has a rich ecosystem
of libraries and frameworks for a wide range of applications. It is also easy
to learn and use, with a readable and concise syntax and extensive
documentation and resources.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>TensorFlow
is a Python open-source library created by the Google Brain team. TensorFlow
combines many different algorithms and models, allowing users to build deep
neural networks for tasks such as image recognition/classification and natural
language processing. TensorFlow is a powerful framework that functions by
implementing a series of processing nodes, each node representing a
mathematical operation, with the entire series of nodes being called a
&quot;graph&quot;. It provides a wide range of tools and libraries for
building, training, and evaluating machine learning models. It is designed to
be highly scalable and flexible, and it can be used for a variety of tasks,
including image classification, speech recognition, and natural language
processing. By using Python in Google Colab, we can easily and efficiently
develop and test the speech emotion classification model using image
classification and spectrogram in TensorFlow without having to install and set
up the required software and hardware on your local machine. This can save time
and resources, and it allows us to easily collaborate with others and share the
work.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Keras
is a high-level API (application programming interface) that can make use of
TensorFlow's functions. Keras was created with the principles of usability and
modularity in mind. In practice, Keras makes implementing TensorFlow's many
powerful but often complex functions as simple as possible, and it's set up to
work with Python without any major changes or configuration. Keras provides a
simple and intuitive interface for building, training, and evaluating machine learning
models, which can be particularly useful for prototyping and experimentation.
It allows you to define the layers and connections of a model using a few lines
of code, and it provides a variety of loss functions, optimizers, and metrics
for training and evaluation.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><b><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>TensorFlow
IO (TFIO) Modules used and their functions</span></b></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>tf.cast
is a TensorFlow function that casts a tensor to a new type. It takes a tensor
and a new type as input, and it returns a tensor with the same shape as the
input tensor and the specified new type.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>tfio.audio.trim
is a TensorFlow IO (TFIO) function for trimming an audio signal to a specified
duration. It takes an audio signal and a duration as input, and it returns a
new audio signal with the specified duration. It also provides various options
and parameters for customizing the trimming of the audio signal, such as the
start time and end time.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>tfio.audio.fade
is a TensorFlow IO (TFIO) function that allows you to apply a fade-in or
fade-out effect to an audio signal. It takes an audio signal and a fade type as
input and outputs a new audio signal with the fade effect applied. It also
includes options and parameters for customising the fade effect, such as start
time, end time, and fade duration.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>tfio.audio.spectrogram
accepts an audio signal as input and returns the spectrogram as a
two-dimensional tensor, with the first dimension representing time frames and
the second dimension representing frequency bins. It also includes various
options and parameters for customising the spectrogram calculation, such as
window size, hop size, and window type.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>tfio.audio.melscale
The TensorFlow IO's melscale function converts a spectrogram to the Mel
frequency scale. The Mel scale is a frequency representation scale based on how
frequencies are perceived by the human auditory system. Because it is more
closely aligned with how humans perceive sound than the standard frequency
scale, it is commonly used in speech and audio processing.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>tfio.audio
dbscale function takes a spectrogram as input and returns a tensor of the same
shape with the values of the spectrogram expressed in dB. The decibel scale is
a logarithmic scale used to express the ratio of one value to another. In audio
processing, it is used to express the power or intensity of a signal relative
to a reference level.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><a
name="_Hlk123862269"><b><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'>&nbsp;</span></b></a></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><b><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>4
- Data components, preprocessing and transformation</span></b></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><a
name="_Hlk123863389"><b><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'>4.1 - Data Source</span></b></a></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>The
dataset for this project, titled ' RAVDESS Emotional speech audio,' was
obtained from Kaggle. Kaggle is a data science community where expert and
novice data scientists can collaborate and share their findings. The data was
downloaded 22724 times from Kaggle. RAVDESS (Ryerson Audio-Visual Database of
Emotional Speech and Song) is a multimodal library of songs and speeches with
strong emotions. It comprises speech and music records that have been
classified into eight distinct categories by 247 untrained Americans: Relaxed,
Happy, Sad, Furious, Afraid, Disgust, and Surprise, as well as a neutral basis
for each performance. The data is collected by 24 trained performers, 12 males
and 12 females. Gender-balanced. The audio files were created in a standardised
context and feature the same statements with an American accent. There are two
other file formats that are related:&nbsp;There are 1440 files in the speech
file: 60 trials per actor multiplied by 24 actors is 1440 voice files (audio
speech actor 01-24.zip, 215 MB). The audio file contains the following 1012
files (audio song actors 01-24.zip, 198 MB): 44 actor ratings multiplied by 23
actors equals 1012. In the WAV raw audio file format, both types of audio files
have a 16-bit bitrate and a sampling rate of 48 kHz. Both recordings are
uncommented, lossless audio, guaranteeing that no data/information has been
lost or modified since the audio files in the dataset were first recorded.
Livingstone and Russo (2018).</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><b><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>4.2
<a name="_Hlk123863468">- </a><a name="_Hlk123862499">Data Preparation and
Augmentation</a></span></b></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>The
preparation and augmentation of audio data are one of the most difficult
challenges in Automatic Speech Recognition. For that, I first modified the
dataset in 8 classes of emotion and saved them in google drive. Then for trial
purposes augmentation of one sample audio file was carried out. For that, the
TensorFlow ecosystem was used. the tensorflow-io package includes a number of
useful audio-related APIs that aid in the preparation and enhancement of audio
data. In TensorFlow IO, class tfio.audio.AudioIOTensor allows us to read an
audio file into a lazy-loaded IOTensor. The audio file can be converted into
float numbers and showing the audio clip in a graph is more convenient</span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=429 height=86 id="Picture 29"
src="Speech%20Emotion%20Classification.files/image007.png"
alt="A picture containing text&#10;&#10;Description automatically generated"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><a name="_Hlk123864043"><i><span style='font-size:14.0pt;
font-family:"Times New Roman",serif;color:#44546A'>Figure 4.1 Cast Function</span></i></a></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=391 height=243 id="Picture 2"
src="Speech%20Emotion%20Classification.files/image008.png"
alt="A picture containing text, airplane, aircraft&#10;&#10;Description automatically generated"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><a name="_Hlk123864057"><i><span style='font-size:14.0pt;
font-family:"Times New Roman",serif;color:#44546A'>Figure 4.2 Wave form of
sample file after Cast Function</span></i></a></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Then
the audio was trimmed in order to remove the noise, which was done through API
tfio.audio.trim. Returned from the API is a pair of [start, stop] positions of
the segment. tfio.audio.trim is used to remove unwanted or unnecessary parts of
an audio signal, such as silences or background noise, or to extract a specific
segment of an audio signal for further processing or analysis</span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=454 height=204 id="Picture 11"
src="Speech%20Emotion%20Classification.files/image009.png"
alt="Graphical user interface, text, application&#10;&#10;Description automatically generated"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><a name="_Hlk123864099"><i><span style='font-size:14.0pt;
font-family:"Times New Roman",serif;color:#44546A'>Figure 4.3 Trim Function</span></i></a></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><a name="_Hlk123864124"><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'><img width=375
height=231 id="Picture 3"
src="Speech%20Emotion%20Classification.files/image010.png"
alt="A silhouette of a city&#10;&#10;Description automatically generated with low confidence"></span></a></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><i><span style='font-size:14.0pt;font-family:"Times New Roman",serif;
color:#44546A'>Figure 4.4 Audio file after Trim Function</span></i></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>tfio.audio.fade
is used to smoothly transition between audio signals or to reduce the volume of
an audio signal at the beginning or end. It can also be used to improve the
aesthetics or clarity of an audio signal by removing abrupt changes in volume
or by smoothing transitions between segments. Fade is a useful audio
engineering technique that gradually increases or decreases audio signals. This
is possible with tfio.audio.fade, which supports various fade shapes such as
linear, logarithmic, and exponential. Logarithmic was used here</span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=489 height=100 id="Picture 12"
src="Speech%20Emotion%20Classification.files/image011.png"
alt="A picture containing graphical user interface&#10;&#10;Description automatically generated"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><a name="_Hlk123864194"><i><span style='font-size:14.0pt;
font-family:"Times New Roman",serif;color:#44546A'>Figure 4.5 Fade Function</span></i></a></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=402 height=248 id="Picture 4"
src="Speech%20Emotion%20Classification.files/image012.png"
alt="A picture containing text, airplane, aircraft&#10;&#10;Description automatically generated"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><a name="_Hlk123864215"><i><span style='font-size:14.0pt;
font-family:"Times New Roman",serif;color:#44546A'>Figure 4.6 Audio file after Fade
Function</span></i></a></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><i><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Spectrogram:</span></i><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'> Frequency
changes over time are frequently used in advanced audio processing. A waveform
in tensorflow-io can be converted to a spectrogram using
tfio.audio.spectrogram:</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>tfio.audio.spectrogram
is a TensorFlow IO (TFIO) module for calculating the spectrogram of an audio
signal. tfio.audio.spectrogram takes an audio signal as input and returns its
spectrogram as a 2D tensor, with the first dimension representing the time
frames and the second dimension representing the frequency bins. It also
provides various options and parameters for customizing the calculation of the
spectrogram, such as the window size, hop size, and window type.</span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=333 height=41 id="Picture 16"
src="Speech%20Emotion%20Classification.files/image013.png"
alt="A screenshot of a computer&#10;&#10;Description automatically generated with low confidence"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><a name="_Hlk123864230"><i><span style='font-size:14.0pt;
font-family:"Times New Roman",serif;color:#44546A'>Figure 4.7 Spectrogram
Module</span></i></a></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>The code snippet
calculates the spectrogram of an audio signal stored in a variable fade using the
tfio.audio.spectrogram function. The spectrogram is calculated using a Fast
Fourier Transform (FFT) with a size of 512 samples (nfft=512), a window size of
512 samples (window=512), and a stride of 256 samples (stride=256).</span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=273 height=203 id="Picture 5"
src="Speech%20Emotion%20Classification.files/image014.png"
alt="A picture containing text, monitor, television, screen&#10;&#10;Description automatically generated"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><a name="_Hlk123864245"><i><span style='font-size:14.0pt;
font-family:"Times New Roman",serif;color:#44546A'>Figure 4.8 Spectrogram using
FFT</span></i></a></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Here the rate takes The
sample rate of the audio in Hz. The mels: The number of Mel frequency bands to
use. This determines the resolution of the output spectrogram.</span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=438 height=42 id="Picture 14"
src="Speech%20Emotion%20Classification.files/image015.png"
alt="A picture containing text&#10;&#10;Description automatically generated"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><a name="_Hlk123864261"><i><span style='font-size:14.0pt;
font-family:"Times New Roman",serif;color:#44546A'>Figure 4.9 Mel frequency spectrogram
module .</span></i></a></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Then fmin: The minimum
frequency to include in the Mel spectrogram, in Hz. And fmax: The maximum
frequency to include in the Mel spectrogram, in Hz.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=109 height=252 id="Picture 6"
src="Speech%20Emotion%20Classification.files/image016.png"
alt="A picture containing text, monitor, screen, clock&#10;&#10;Description automatically generated"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><a name="_Hlk123864278"><i><span style='font-size:14.0pt;
font-family:"Times New Roman",serif;color:#44546A'>Figure 4.10 Mel frequency
spectrogram</span></i></a></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'><img
width=356 height=40 id="Picture 15"
src="Speech%20Emotion%20Classification.files/image017.png"
alt="A screenshot of a computer&#10;&#10;Description automatically generated with low confidence"></span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=109 height=252 id="Picture 7"
src="Speech%20Emotion%20Classification.files/image016.png"
alt="A picture containing text, monitor, screen, clock&#10;&#10;Description automatically generated"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><a name="_Hlk123864301"><i><span style='font-size:14.0pt;
font-family:"Times New Roman",serif;color:#44546A'>Figure 4.11 dbscale Mel
frequency spectrogram</span></i></a></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Here, the dbscale
function is being called with the parameter top_db=120, which means that values
in the output spectrogram will be capped at 120 dB. Any values in the input
spectrogram that are above this threshold will be set to 120 dB in the output
spectrogram</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><i><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>SpecAugment:</span></i><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'> In
addition to the data preparation and augmentation APIs mentioned above, the
tensorflow-io package offers advanced spectrogram augmentations, most notably
Frequency and Time Masking, which are discussed in SpecAugment: A Simple Data
Augmentation Method for Automatic Speech Recognition (Park et al., 2019).Using
Frequency Masking: Frequency channels [f0, f0 + f) are masked in frequency
masking, where f is chosen from a uniform distribution from 0 to the frequency
mask parameter F, and f0 is chosen from (0, v f) where is the number of
frequency channels. </span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'><img
width=520 height=21 id="Picture 17"
src="Speech%20Emotion%20Classification.files/image018.png"></span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=109 height=252 id="Picture 9"
src="Speech%20Emotion%20Classification.files/image019.png"
alt="Background pattern&#10;&#10;Description automatically generated with medium confidence"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><a name="_Hlk123864316"><i><span style='font-size:14.0pt;
font-family:"Times New Roman",serif;color:#44546A'>Figure 4.12 dbscale Mel
frequency spectrogram after freqmask</span></i></a></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Time
masking involves masking t consecutive time steps [t0, t0 + t) where t is
chosen from a uniform distribution from 0 to the time mask parameter T and t0
is chosen from [0, t) where is the time steps.</span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'><img
width=519 height=22 id="Picture 18"
src="Speech%20Emotion%20Classification.files/image020.png"></span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=109 height=252 id="Picture 10"
src="Speech%20Emotion%20Classification.files/image021.png"
alt="A picture containing text, monitor, screen, television&#10;&#10;Description automatically generated"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><a name="_Hlk123864354"><i><span style='font-size:14.0pt;
font-family:"Times New Roman",serif;color:#44546A'>Figure 4.13 dbscale Mel
frequency spectrogram after timemask</span></i></a></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><b><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>&nbsp;</span></b></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><a
name="_Hlk123863504"><b><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'>4.3 - Data Transformation</span></b></a></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Just
like the above procedure, every file in the dataset is converted into dB scale
Mel spectrogram by combining the codes</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>The
complete dataset was then transformed into 8 classes of emotion which will be
used in the model creation. Then each audio file was transformed to a dB scale
Mel spectrogram and saved in google drive.</span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=239 height=220 id="Picture 55"
src="Speech%20Emotion%20Classification.files/image022.png"
alt="Graphical user interface&#10;&#10;Description automatically generated"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><a name="_Hlk123864388"><i><span style='font-size:14.0pt;
font-family:"Times New Roman",serif;color:#44546A'>Figure 4.14 dataset saved
as 8 classes </span></i></a></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=602 height=404 id="Picture 8"
src="Speech%20Emotion%20Classification.files/image023.png"
alt="Graphical user interface&#10;&#10;Description automatically generated with medium confidence"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><a name="_Hlk123864404"><i><span style='font-size:14.0pt;
font-family:"Times New Roman",serif;color:#44546A'>Figure 4.15 Saved files in
the db_spec/calm folder</span></i></a></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><b><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>5
<a name="_Hlk123862854">- Model Implementation</a></span></b></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Image
classification codes from the TensorFlow core are modified for the execution of
the model implementation stage. In this stage, I followed the basic machine
learning workflow. It has six steps, which are as follows 1, Examine and
understand data 2, Build an input pipeline 3, Build the model 4, Train the
model 5, Test the model 6, Improve the model and repeat the process</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><i><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Examine
and understand the data:</span></i><span style='font-size:14.0pt;line-height:
150%;font-family:"Times New Roman",serif'>The pre-processed data was fed into
the TensorFlow workspace. 1440 spectrogram images were there in the
pre-processed db_spec data folder. For building the model the files are loaded
using tf.keras.utils.image_dataset_from_directory utility. This will take
images from a directory of images on disk to a tf.data.Dataset. I followed the
general practice to use a validation split during the model development. 80% of
the images for training and 20% for validation. Using 1152 for training and 288
files for validation from the 1440 files belonging to 8 classes.</span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=602 height=81 id="Picture 13"
src="Speech%20Emotion%20Classification.files/image024.png"
alt="Graphical user interface&#10;&#10;Description automatically generated with low confidence"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><a name="_Hlk123864424"><i><span style='font-size:14.0pt;
font-family:"Times New Roman",serif;color:#44546A'>Figure 5.1 Classes Names</span></i></a></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Then
the first 9 images from the training dataset were visualised.</span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=402 height=406 id="Picture 19"
src="Speech%20Emotion%20Classification.files/image025.png"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><a name="_Hlk123859362"><i><span style='font-size:14.0pt;
font-family:"Times New Roman",serif;color:#44546A'>Figure 5.2 Sample of
training dataset visualised </span></i></a></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><i><span style='font-size:14.0pt;font-family:"Times New Roman",serif;
color:#44546A'>&nbsp;</span></i></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Configure
the dataset for performance: The dataset was configured for performance using
buffered prefetching. For that two import methods were followed. Dataset.cache
After the images are loaded from the disc during the first epoch, they are kept
in memory. This will prevent the dataset from becoming a bottleneck while
training your model. This method was used to build a fast on-disk cache.
Dataset.prefetch combines data preprocessing and model execution while
training. </span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=569 height=79 id="Picture 30"
src="Speech%20Emotion%20Classification.files/image026.png"
alt="A picture containing logo&#10;&#10;Description automatically generated"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><a name="_Hlk123864463"><i><span style='font-size:14.0pt;
font-family:"Times New Roman",serif;color:#44546A'>Figure 5.3 Prefetching</span></i></a></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><i><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Standardize
the data</span></i><span style='font-size:14.0pt;line-height:150%;font-family:
"Times New Roman",serif'>: The RGB values are in the [0, 255] range. This is
not ideal for a neural network, so it was standardised to be in the [0, 1]
range using tf.keras.layers.Rescaling.</span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=366 height=30 id="Picture 31"
src="Speech%20Emotion%20Classification.files/image027.png"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><i><span style='font-size:14.0pt;font-family:"Times New Roman",serif;
color:#44546A'>Figure 5.4 Normalisation</span></i></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><i><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Designing
the Model</span></i></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>This
is the stage at which we design the CNN model. The first step is to define the
format for the model. Keras has several different formats or blueprints to
build models on, but Sequential is the most commonly used, so we imported it
from Keras</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><i><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Create
the model</span></i></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Our
model's first layer is a convolutional layer. It will accept inputs and apply
convolutional filters to them. When we implement these in Keras, we must
specify the number of channels/filters, the size of the filter, the input shape
(when creating the first layer), and the activation and padding we require.The
Keras Sequential model is made up of three convolution blocks
(tf.keras.layers.Conv2D) each with a maximum pooling layer
(tf.keras.layers.MaxPooling2D). Each pooling discards some data by slashing the
input dimensions by a given factor. After the convolutional layers, we need to
Flatten the data. A ReLU activation function ('relu') activates a
fully-connected layer (tf.keras.layers.Dense) with 128 units on top of it. relu
is the most common activation, and padding='same' just means we aren't changing
the size of the image at all. Then, using the Dense import, we created the
first densely connected layer. The number of neurons in the dense layer must be
specified. It is worth noting that the number of neurons in subsequent layers
decreases, eventually approaching the same number as the number of classes in
the dataset.</span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=499 height=258 id="Picture 20"
src="Speech%20Emotion%20Classification.files/image028.png"
alt="A picture containing table&#10;&#10;Description automatically generated"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><a name="_Hlk123888382"><i><span style='font-size:14.0pt;
font-family:"Times New Roman",serif;color:#44546A'>Figure 5.5 Model creation </span></i></a></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Compile
the model: We just compiled the model we want to use now that we've designed
it. The optimizer is responsible for adjusting the weights in your network to
approach the point of least loss. The Adaptive Moment Estimation (Adam)
algorithm is a popular optimizer and a good default optimizer. The
tf.keras.optimizers.Adam&nbsp;optimizer
and&nbsp;tf.keras.losses.SparseCategoricalCrossentropy&nbsp;loss function were
used . To view training and validation accuracy for each training epoch, pass
the&nbsp;metrics&nbsp;argument to&nbsp;Model.compile.</span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=602 height=61 id="Picture 21"
src="Speech%20Emotion%20Classification.files/image029.png"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><i><span style='font-size:14.0pt;font-family:"Times New Roman",serif;
color:#44546A'>Figure 5.6 Model Compile </span></i></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Next
the model summary was printed to see what the whole model look like</span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=366 height=298 id="Picture 32"
src="Speech%20Emotion%20Classification.files/image030.png"
alt="Table&#10;&#10;Description automatically generated"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><i><span style='font-size:14.0pt;font-family:"Times New Roman",serif;
color:#44546A'>Figure 5.7 Model Summary</span></i></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><b><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>6
<a name="_Hlk123862953">- </a><a name="_Hlk123862913"></a><a
name="_Hlk123727806_0">Results, Testing and Analysis</a></span></b></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><a
name="_Hlk123863142"><b><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'>6.1 - Results </span></b></a></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>After
implementing the model we need to train the model. The model was trained for 10
epochs with the Keras Model.fit module.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=405 height=121 id="Picture 22"
src="Speech%20Emotion%20Classification.files/image031.png"
alt="Text, letter&#10;&#10;Description automatically generated"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><a name="_Hlk123888465"><i><span style='font-size:14.0pt;
font-family:"Times New Roman",serif;color:#44546A'>Figure 6.1 Model Training</span></i></a></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>The
following results were obtained</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=606 height=215 id="Picture 33"
src="Speech%20Emotion%20Classification.files/image032.png"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><i><span style='font-size:14.0pt;font-family:"Times New Roman",serif;
color:#44546A'>Figure 6.2 Model Training Results</span></i></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>The
training results were visualised </span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=478 height=364 id="Picture 23"
src="Speech%20Emotion%20Classification.files/image033.png"
alt="Text&#10;&#10;Description automatically generated"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><a name="_Hlk123888508"><i><span style='font-size:14.0pt;
font-family:"Times New Roman",serif;color:#44546A'>Figure 6.3 Visualise Training
Results</span></i></a></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=486 height=485 id="Picture 34"
src="Speech%20Emotion%20Classification.files/image034.png"
alt="Chart, line chart&#10;&#10;Description automatically generated"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><a name="_Hlk123888533"><i><span style='font-size:14.0pt;
font-family:"Times New Roman",serif;color:#44546A'>Figure 6.4 Training and
Validation Accuracy/Loss </span></i></a></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><b><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>6.2
<a name="_Hlk123863157">- Testing </a></span></b></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>The
results show almost 90% training accuracy but the best validation accuracy is
approximately 54%.In order to get better results parameters like epochs,
learning rate and batch size are changed in the following test results. The
default learning rate in adam optimiser is 0.001, and a different learning rate
of 0.0001 was tested. Different batch sizes 8. 16,32, 64 etc were also tested.
The epochs were also changed in order to find the best-performing model.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>The
following pages contain the results and summary of also the test performed and
their training accuracy, validation accuracy, training loss and validation loss
in percentage.</span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=399 height=398 id="Picture 35"
src="Speech%20Emotion%20Classification.files/image035.png"
alt="Chart, line chart&#10;&#10;Description automatically generated"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><a name="_Hlk123888568"><i><span style='font-size:14.0pt;
font-family:"Times New Roman",serif;color:#44546A'>Figure 6.5 Training and
Validation Accuracy/Loss - learning rate -0.0001, batch size =32, epochs =10</span></i></a></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=399 height=398 id="Picture 36"
src="Speech%20Emotion%20Classification.files/image036.png"
alt="Chart&#10;&#10;Description automatically generated"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><i><span style='font-size:14.0pt;font-family:"Times New Roman",serif;
color:#44546A'>Figure 6.6 Training and Validation Accuracy/Loss - learning rate
-0.001, batch size =32, epochs =15</span></i></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=428 height=427 id="Picture 37"
src="Speech%20Emotion%20Classification.files/image037.png"
alt="Chart, line chart&#10;&#10;Description automatically generated"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><i><span style='font-size:14.0pt;font-family:"Times New Roman",serif;
color:#44546A'>Figure 6.7 Training and Validation Accuracy/Loss - learning rate
-0.001, batch size =16, epochs =10</span></i></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=410 height=409 id="Picture 38"
src="Speech%20Emotion%20Classification.files/image038.png"
alt="Chart&#10;&#10;Description automatically generated with medium confidence"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><i><span style='font-size:14.0pt;font-family:"Times New Roman",serif;
color:#44546A'>Figure 6.8 Training and Validation Accuracy/Loss - learning rate
-0.001, batch size =16, epochs =15</span></i></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=436 height=436 id="Picture 40"
src="Speech%20Emotion%20Classification.files/image039.png"
alt="Chart, line chart&#10;&#10;Description automatically generated"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><i><span style='font-size:14.0pt;font-family:"Times New Roman",serif;
color:#44546A'>Figure 6.9 Training and Validation Accuracy/Loss - learning rate
-0.001, batch size =8, epochs =15</span></i></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=398 height=398 id="Picture 41"
src="Speech%20Emotion%20Classification.files/image040.png"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><i><span style='font-size:14.0pt;font-family:"Times New Roman",serif;
color:#44546A'>Figure 6.10 Training and Validation Accuracy/Loss - learning
rate -0.001, batch size =64, epochs =15</span></i></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>&nbsp;</span></p>

<div align=center>

<table class=MsoTableGrid border=1 cellspacing=0 cellpadding=0
 style='border-collapse:collapse;border:none'>
 <tr style='height:40.8pt'>
  <td width=73 valign=top style='width:54.65pt;border:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:40.8pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>Test</span></p>
  </td>
  <td width=73 nowrap valign=top style='width:54.65pt;border:solid windowtext 1.0pt;
  border-left:none;padding:0cm 5.4pt 0cm 5.4pt;height:40.8pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>batch size</span></p>
  </td>
  <td width=137 nowrap valign=top style='width:102.5pt;border:solid windowtext 1.0pt;
  border-left:none;padding:0cm 5.4pt 0cm 5.4pt;height:40.8pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>learning rate</span></p>
  </td>
  <td width=137 nowrap valign=top style='width:102.5pt;border:solid windowtext 1.0pt;
  border-left:none;padding:0cm 5.4pt 0cm 5.4pt;height:40.8pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>epochs</span></p>
  </td>
  <td width=84 nowrap valign=top style='width:62.75pt;border:solid windowtext 1.0pt;
  border-left:none;padding:0cm 5.4pt 0cm 5.4pt;height:40.8pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>Highest Training accuracy</span></p>
  </td>
  <td width=90 nowrap valign=top style='width:67.55pt;border:solid windowtext 1.0pt;
  border-left:none;padding:0cm 5.4pt 0cm 5.4pt;height:40.8pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>Highest Validation accuracy</span></p>
  </td>
 </tr>
 <tr style='height:8.55pt'>
  <td width=73 valign=top style='width:54.65pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0cm 5.4pt 0cm 5.4pt;height:8.55pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>1</span></p>
  </td>
  <td width=73 nowrap valign=top style='width:54.65pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:8.55pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>32</span></p>
  </td>
  <td width=137 nowrap valign=top style='width:102.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:8.55pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>0.001</span></p>
  </td>
  <td width=137 nowrap valign=top style='width:102.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:8.55pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>10</span></p>
  </td>
  <td width=84 nowrap valign=top style='width:62.75pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:8.55pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>93.84</span></p>
  </td>
  <td width=90 nowrap valign=top style='width:67.55pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:8.55pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>54.17</span></p>
  </td>
 </tr>
 <tr style='height:11.9pt'>
  <td width=73 valign=top style='width:54.65pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>2</span></p>
  </td>
  <td width=73 nowrap valign=top style='width:54.65pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>64</span></p>
  </td>
  <td width=137 nowrap valign=top style='width:102.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>0.001</span></p>
  </td>
  <td width=137 nowrap valign=top style='width:102.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>15</span></p>
  </td>
  <td width=84 nowrap valign=top style='width:62.75pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>95.83</span></p>
  </td>
  <td width=90 nowrap valign=top style='width:67.55pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>53.82</span></p>
  </td>
 </tr>
 <tr style='height:11.9pt'>
  <td width=73 valign=top style='width:54.65pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>3</span></p>
  </td>
  <td width=73 nowrap valign=top style='width:54.65pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>32</span></p>
  </td>
  <td width=137 nowrap valign=top style='width:102.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>0.0001</span></p>
  </td>
  <td width=137 nowrap valign=top style='width:102.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>10</span></p>
  </td>
  <td width=84 nowrap valign=top style='width:62.75pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>62.85</span></p>
  </td>
  <td width=90 nowrap valign=top style='width:67.55pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>46.53</span></p>
  </td>
 </tr>
 <tr style='height:11.9pt'>
  <td width=73 valign=top style='width:54.65pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>4</span></p>
  </td>
  <td width=73 nowrap valign=top style='width:54.65pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>32</span></p>
  </td>
  <td width=137 nowrap valign=top style='width:102.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>0.001</span></p>
  </td>
  <td width=137 nowrap valign=top style='width:102.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>15</span></p>
  </td>
  <td width=84 nowrap valign=top style='width:62.75pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>99.91</span></p>
  </td>
  <td width=90 nowrap valign=top style='width:67.55pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>52.08</span></p>
  </td>
 </tr>
 <tr style='height:11.9pt'>
  <td width=73 valign=top style='width:54.65pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>5</span></p>
  </td>
  <td width=73 nowrap valign=top style='width:54.65pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>16</span></p>
  </td>
  <td width=137 nowrap valign=top style='width:102.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>0.001</span></p>
  </td>
  <td width=137 nowrap valign=top style='width:102.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>10</span></p>
  </td>
  <td width=84 nowrap valign=top style='width:62.75pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>95.49</span></p>
  </td>
  <td width=90 nowrap valign=top style='width:67.55pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>51.74</span></p>
  </td>
 </tr>
 <tr style='height:11.9pt'>
  <td width=73 valign=top style='width:54.65pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>6</span></p>
  </td>
  <td width=73 nowrap valign=top style='width:54.65pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>16</span></p>
  </td>
  <td width=137 nowrap valign=top style='width:102.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>0.001</span></p>
  </td>
  <td width=137 nowrap valign=top style='width:102.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>15</span></p>
  </td>
  <td width=84 nowrap valign=top style='width:62.75pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>100</span></p>
  </td>
  <td width=90 nowrap valign=top style='width:67.55pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>58.68</span></p>
  </td>
 </tr>
 <tr style='height:11.9pt'>
  <td width=73 valign=top style='width:54.65pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>7</span></p>
  </td>
  <td width=73 nowrap valign=top style='width:54.65pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>8</span></p>
  </td>
  <td width=137 nowrap valign=top style='width:102.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>0.001</span></p>
  </td>
  <td width=137 nowrap valign=top style='width:102.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>10</span></p>
  </td>
  <td width=84 nowrap valign=top style='width:62.75pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>95.05</span></p>
  </td>
  <td width=90 nowrap valign=top style='width:67.55pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:11.9pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>48.26</span></p>
  </td>
 </tr>
 <tr style='height:2.85pt'>
  <td width=73 valign=top style='width:54.65pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0cm 5.4pt 0cm 5.4pt;height:2.85pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>8</span></p>
  </td>
  <td width=73 nowrap valign=top style='width:54.65pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:2.85pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>8</span></p>
  </td>
  <td width=137 nowrap valign=top style='width:102.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:2.85pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>0.001</span></p>
  </td>
  <td width=137 nowrap valign=top style='width:102.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:2.85pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>15</span></p>
  </td>
  <td width=84 nowrap valign=top style='width:62.75pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:2.85pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%'><span style='font-size:14.0pt;line-height:150%;font-family:
  "Times New Roman",serif'>99.13</span></p>
  </td>
  <td width=90 nowrap valign=top style='width:67.55pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:2.85pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:150%;page-break-after:avoid'><span style='font-size:14.0pt;
  line-height:150%;font-family:"Times New Roman",serif'>53.12</span></p>
  </td>
 </tr>
</table>

</div>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><a name="_Hlk123889340"><i><span style='font-size:14.0pt;
font-family:"Times New Roman",serif;color:#44546A'>Table 6.1 Summary of Test
Results</span></i></a></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>The
training accuracy increases linearly over time in the plots above, whereas the
validation accuracy plateaus around 58% during the training process.
Furthermore, there is a noticeable difference in accuracy between training and
validation accuracy, indicating overfitting. When there are a limited number of
training examples, the model may learn from noises or unwanted details in the
training examples, which can have a negative impact on the model's performance
on new examples. This is referred to as overfitting. It means that the model
will struggle to generalise to a new dataset. There are several methods for
combating overfitting during the training process. Here I used data
augmentation and dropout to the model.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Data
augmentation: Overfitting occurs when there are a limited number of training
examples. The approach of data augmentation is to generate additional training
data from your existing examples by augmenting them with random transformations
that produce believable-looking images. This allows the model to be exposed to
more aspects of the data and generalise more effectively.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Dropout:
Dropout regularisation is another method for reducing overfitting in networks.
When dropout is applied to a layer, it randomly drops out (by setting the
activation to zero) a number of output units from the layer during the training
process. Dropout accepts a fractional number as input, such as 0.1, 0.2, 0.4,
and so on. This entails randomly selecting 10%, 20%, or 40% of the output units
from the applied layer.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>In
addition to that a different optimiser nadam (Nesterov Adaptive Moment
Estimation) was also tested instead of the adam (Adam (Adaptive Moment
Estimation) optimizer. The following code was used of the model.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=479 height=251 id="Picture 24"
src="Speech%20Emotion%20Classification.files/image041.png"
alt="A picture containing table&#10;&#10;Description automatically generated"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><a name="_Hlk123888877"><i><span style='font-size:14.0pt;
font-family:"Times New Roman",serif;color:#44546A'>Figure 6.11 Drop out</span></i></a></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>The following results were
obtained after applying spec augment and dropout. </span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=377 height=376 id="Picture 45"
src="Speech%20Emotion%20Classification.files/image042.png"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><i><span style='font-size:14.0pt;font-family:"Times New Roman",serif;
color:#44546A'>Figure 6.12 Training and <a name="_Hlk123888907">Validation
Accuracy/Loss - optimizer = adam, batch size =32, epochs =15, dropout =0.2</a></span></i></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=390 height=390 id="Picture 46"
src="Speech%20Emotion%20Classification.files/image043.png"
alt="Chart&#10;&#10;Description automatically generated"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><i><span style='font-size:14.0pt;font-family:"Times New Roman",serif;
color:#44546A'>Figure 6.13 Training and Validation Accuracy/Loss - optimizer =
adam, batch size =16, epochs =15, dropout =0.2</span></i></p>

<p class=MsoNormal><span style='font-size:14.0pt;line-height:107%;font-family:
"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal align=center style='text-align:center;line-height:150%;
page-break-after:avoid'><span style='font-size:14.0pt;line-height:150%;
font-family:"Times New Roman",serif'><img width=417 height=416 id="Picture 47"
src="Speech%20Emotion%20Classification.files/image044.png"
alt="Chart&#10;&#10;Description automatically generated"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><i><span style='font-size:14.0pt;font-family:"Times New Roman",serif;
color:#44546A'>Figure 6.14 Training and Validation Accuracy/Loss - optimizer =
adam, batch size =16, epochs =15, dropout =0.4</span></i></p>

<p class=MsoNormal align=center style='text-align:center;page-break-after:avoid'><span
style='font-size:14.0pt;line-height:107%;font-family:"Times New Roman",serif'><img
width=412 height=412 id="Picture 48"
src="Speech%20Emotion%20Classification.files/image045.png"
alt="A picture containing diagram&#10;&#10;Description automatically generated"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><i><span style='font-size:14.0pt;font-family:"Times New Roman",serif;
color:#44546A'>Figure 6.15 Training and Validation Accuracy/Loss - optimizer =
adam, batch size =16, epochs =20, dropout =0.4</span></i></p>

<p class=MsoNormal align=center style='text-align:center;page-break-after:avoid'><span
style='font-size:14.0pt;line-height:107%;font-family:"Times New Roman",serif'><img
width=436 height=435 id="Picture 49"
src="Speech%20Emotion%20Classification.files/image046.png"
alt="Diagram&#10;&#10;Description automatically generated with medium confidence"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><i><span style='font-size:14.0pt;font-family:"Times New Roman",serif;
color:#44546A'>Figure 6.16 Training and Validation Accuracy/Loss - optimizer =
adam, batch size =32, epochs =15, dropout =0.5</span></i></p>

<p class=MsoNormal align=center style='text-align:center;page-break-after:avoid'><span
style='font-size:14.0pt;line-height:107%;font-family:"Times New Roman",serif'><img
width=407 height=407 id="Picture 50"
src="Speech%20Emotion%20Classification.files/image047.png"
alt="Diagram&#10;&#10;Description automatically generated with low confidence"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><i><span style='font-size:14.0pt;font-family:"Times New Roman",serif;
color:#44546A'>Figure 6.17 Training and Validation Accuracy/Loss - optimizer =
adam, batch size =32, epochs =20, dropout =0.5</span></i></p>

<p class=MsoNormal align=center style='text-align:center;page-break-after:avoid'><span
style='font-size:14.0pt;line-height:107%;font-family:"Times New Roman",serif'><img
width=441 height=440 id="Picture 51"
src="Speech%20Emotion%20Classification.files/image048.png"
alt="Chart&#10;&#10;Description automatically generated"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><i><span style='font-size:14.0pt;font-family:"Times New Roman",serif;
color:#44546A'>Figure 6.18 Training and Validation Accuracy/Loss - optimizer =nadam,
batch size =32, epochs =15, dropout =0.4</span></i></p>

<p class=MsoNormal align=center style='text-align:center;page-break-after:avoid'><span
style='font-size:14.0pt;line-height:107%;font-family:"Times New Roman",serif'><img
width=414 height=414 id="Picture 52"
src="Speech%20Emotion%20Classification.files/image049.png"
alt="Chart, line chart&#10;&#10;Description automatically generated"></span></p>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><i><span style='font-size:14.0pt;font-family:"Times New Roman",serif;
color:#44546A'>Figure 6.19 Training and Validation Accuracy/Loss - optimizer =nadam,
batch size =32, epochs =15, dropout =0.5</span></i></p>

<p class=MsoNormal><span style='font-size:14.0pt;line-height:107%;font-family:
"Times New Roman",serif'>&nbsp;</span></p>

<div align=center>

<table class=MsoTableGrid border=1 cellspacing=0 cellpadding=0 width=616
 style='width:461.7pt;border-collapse:collapse;border:none'>
 <tr style='height:12.15pt'>
  <td width=69 nowrap valign=top style='width:51.5pt;border:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>Test</span></p>
  </td>
  <td width=129 nowrap valign=top style='width:96.65pt;border:solid windowtext 1.0pt;
  border-left:none;padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>optimiser</span></p>
  </td>
  <td width=129 nowrap valign=top style='width:96.65pt;border:solid windowtext 1.0pt;
  border-left:none;padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>batch
  size</span></p>
  </td>
  <td width=69 nowrap valign=top style='width:51.5pt;border:solid windowtext 1.0pt;
  border-left:none;padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>epochs</span></p>
  </td>
  <td width=69 nowrap valign=top style='width:51.5pt;border:solid windowtext 1.0pt;
  border-left:none;padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>dropout</span></p>
  </td>
  <td width=71 nowrap valign=top style='width:53.55pt;border:solid windowtext 1.0pt;
  border-left:none;padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>Highest
  Training accuracy</span></p>
  </td>
  <td width=80 nowrap valign=top style='width:60.35pt;border:solid windowtext 1.0pt;
  border-left:none;padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>Highest
  Validation accuracy</span></p>
  </td>
 </tr>
 <tr style='height:12.15pt'>
  <td width=69 nowrap valign=top style='width:51.5pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>1</span></p>
  </td>
  <td width=129 nowrap valign=top style='width:96.65pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>adam</span></p>
  </td>
  <td width=129 nowrap valign=top style='width:96.65pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>32</span></p>
  </td>
  <td width=69 nowrap valign=top style='width:51.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>15</span></p>
  </td>
  <td width=69 nowrap valign=top style='width:51.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>0.2</span></p>
  </td>
  <td width=71 nowrap valign=top style='width:53.55pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>56.6</span></p>
  </td>
  <td width=80 nowrap valign=top style='width:60.35pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>41.32</span></p>
  </td>
 </tr>
 <tr style='height:12.15pt'>
  <td width=69 nowrap valign=top style='width:51.5pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>2</span></p>
  </td>
  <td width=129 nowrap valign=top style='width:96.65pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>adam</span></p>
  </td>
  <td width=129 nowrap valign=top style='width:96.65pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>16</span></p>
  </td>
  <td width=69 nowrap valign=top style='width:51.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>15</span></p>
  </td>
  <td width=69 nowrap valign=top style='width:51.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>0.2</span></p>
  </td>
  <td width=71 nowrap valign=top style='width:53.55pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>77.34</span></p>
  </td>
  <td width=80 nowrap valign=top style='width:60.35pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>51.39</span></p>
  </td>
 </tr>
 <tr style='height:12.15pt'>
  <td width=69 nowrap valign=top style='width:51.5pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>3</span></p>
  </td>
  <td width=129 nowrap valign=top style='width:96.65pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>adam</span></p>
  </td>
  <td width=129 nowrap valign=top style='width:96.65pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>16</span></p>
  </td>
  <td width=69 nowrap valign=top style='width:51.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>15</span></p>
  </td>
  <td width=69 nowrap valign=top style='width:51.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>0.4</span></p>
  </td>
  <td width=71 nowrap valign=top style='width:53.55pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>80.64</span></p>
  </td>
  <td width=80 nowrap valign=top style='width:60.35pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>52.43</span></p>
  </td>
 </tr>
 <tr style='height:12.15pt'>
  <td width=69 nowrap valign=top style='width:51.5pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>4</span></p>
  </td>
  <td width=129 nowrap valign=top style='width:96.65pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>adam</span></p>
  </td>
  <td width=129 nowrap valign=top style='width:96.65pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>16</span></p>
  </td>
  <td width=69 nowrap valign=top style='width:51.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>20</span></p>
  </td>
  <td width=69 nowrap valign=top style='width:51.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>0.4</span></p>
  </td>
  <td width=71 nowrap valign=top style='width:53.55pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>79.34</span></p>
  </td>
  <td width=80 nowrap valign=top style='width:60.35pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>53.82</span></p>
  </td>
 </tr>
 <tr style='height:12.15pt'>
  <td width=69 nowrap valign=top style='width:51.5pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>5</span></p>
  </td>
  <td width=129 nowrap valign=top style='width:96.65pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>adam</span></p>
  </td>
  <td width=129 nowrap valign=top style='width:96.65pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>16</span></p>
  </td>
  <td width=69 nowrap valign=top style='width:51.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>25</span></p>
  </td>
  <td width=69 nowrap valign=top style='width:51.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>0.4</span></p>
  </td>
  <td width=71 nowrap valign=top style='width:53.55pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>90.71</span></p>
  </td>
  <td width=80 nowrap valign=top style='width:60.35pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>51.39</span></p>
  </td>
 </tr>
 <tr style='height:12.15pt'>
  <td width=69 nowrap valign=top style='width:51.5pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>6</span></p>
  </td>
  <td width=129 nowrap valign=top style='width:96.65pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>adam</span></p>
  </td>
  <td width=129 nowrap valign=top style='width:96.65pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>16</span></p>
  </td>
  <td width=69 nowrap valign=top style='width:51.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>15</span></p>
  </td>
  <td width=69 nowrap valign=top style='width:51.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>0.5</span></p>
  </td>
  <td width=71 nowrap valign=top style='width:53.55pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>64.06</span></p>
  </td>
  <td width=80 nowrap valign=top style='width:60.35pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>50</span></p>
  </td>
 </tr>
 <tr style='height:12.15pt'>
  <td width=69 nowrap valign=top style='width:51.5pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>7</span></p>
  </td>
  <td width=129 nowrap valign=top style='width:96.65pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>adam</span></p>
  </td>
  <td width=129 nowrap valign=top style='width:96.65pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>16</span></p>
  </td>
  <td width=69 nowrap valign=top style='width:51.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>20</span></p>
  </td>
  <td width=69 nowrap valign=top style='width:51.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>0.5</span></p>
  </td>
  <td width=71 nowrap valign=top style='width:53.55pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>86.11</span></p>
  </td>
  <td width=80 nowrap valign=top style='width:60.35pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>53.82</span></p>
  </td>
 </tr>
 <tr style='height:12.15pt'>
  <td width=69 nowrap valign=top style='width:51.5pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>8</span></p>
  </td>
  <td width=129 nowrap valign=top style='width:96.65pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>nadam</span></p>
  </td>
  <td width=129 nowrap valign=top style='width:96.65pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>16</span></p>
  </td>
  <td width=69 nowrap valign=top style='width:51.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>15</span></p>
  </td>
  <td width=69 nowrap valign=top style='width:51.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>0.4</span></p>
  </td>
  <td width=71 nowrap valign=top style='width:53.55pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>85.94</span></p>
  </td>
  <td width=80 nowrap valign=top style='width:60.35pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>51.74</span></p>
  </td>
 </tr>
 <tr style='height:12.15pt'>
  <td width=69 nowrap valign=top style='width:51.5pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>9</span></p>
  </td>
  <td width=129 nowrap valign=top style='width:96.65pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>nadam</span></p>
  </td>
  <td width=129 nowrap valign=top style='width:96.65pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>16</span></p>
  </td>
  <td width=69 nowrap valign=top style='width:51.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>15</span></p>
  </td>
  <td width=69 nowrap valign=top style='width:51.5pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>0.5</span></p>
  </td>
  <td width=71 nowrap valign=top style='width:53.55pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><span style='font-size:14.0pt;font-family:"Times New Roman",serif'>76.3</span></p>
  </td>
  <td width=80 nowrap valign=top style='width:60.35pt;border-top:none;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:12.15pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal;page-break-after:avoid'><span style='font-size:14.0pt;
  font-family:"Times New Roman",serif'>52.78</span></p>
  </td>
 </tr>
</table>

</div>

<p class=MsoNormal align=center style='margin-bottom:10.0pt;text-align:center;
line-height:normal'><a name="_Hlk123889315"><i><span style='font-size:14.0pt;
font-family:"Times New Roman",serif;color:#44546A'>Table 6.2 Summary of Test
Results after Spec Augment</span></i></a></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><b><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>&nbsp;</span></b></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><b><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>6.3
<a name="_Hlk123863175">- Performance Analysis</a></span></b></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Even
after testing different hyperparameters the model`s validation accuracy didn`t
improve. If the model has been trained for too long or with too much
complexity, it may have learned patterns that are specific to the training data
but do not generalize well to the validation set. This is known as overfitting
and can lead to low validation accuracy. In order to compensate for this data
augmentation methods like time masks and frequency masks were employed. Dropout
and different optimisers were also tried in order to improve the validation
accuracy percentage. But none of the tests didn`t produce any higher accuracy
values.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>The
validation accuracy of a TensorFlow model is only around 50%, which could
indicate that the model is struggling to accurately classify the data. There
are several potential reasons</span></p>

<p class=MsoNormal style='margin-bottom:0cm;margin-bottom:0cm;margin-top:0cm;
text-align:justify;line-height:150%'><span style='font-size:14.0pt;line-height:
150%;font-family:"Times New Roman",serif'>Insufficient training data: The model
has not been trained on a sufficient amount of data, it may not have learned to
accurately classify the data.</span></p>

<p class=MsoNormal style='margin-bottom:0cm;margin-bottom:0cm;margin-top:0cm;
text-align:justify;line-height:150%'><span style='font-size:14.0pt;line-height:
150%;font-family:"Times New Roman",serif'>Ill-suited model architecture: The
model architecture may not be well-suited for the task, it may not be able to
accurately classify the data.</span></p>

<p class=MsoNormal style='margin-bottom:0cm;margin-bottom:0cm;margin-top:0cm;
text-align:justify;line-height:150%'><span style='font-size:14.0pt;line-height:
150%;font-family:"Times New Roman",serif'>Data imbalance: The training data is
imbalanced, with some classes being much more common than others, the model may
have difficulty learning to classify the underrepresented classes.</span></p>

<p class=MsoNormal style='margin-bottom:0cm;margin-bottom:0cm;margin-top:0cm;
text-align:justify;line-height:150%'><span style='font-size:14.0pt;line-height:
150%;font-family:"Times New Roman",serif'>Hyperparameters: The performance of a
machine learning model can be sensitive to the choice of hyperparameters, such
as the learning rate, batch size, and regularization strength. The
hyperparameters may not be not well-tuned, and the model's performance suffers.</span></p>

<p class=MsoNormal style='margin-bottom:0cm;margin-bottom:0cm;margin-top:0cm;
text-align:justify;line-height:150%'><span style='font-size:14.0pt;line-height:
150%;font-family:"Times New Roman",serif'>Unrepresentative validation set: The
validation set is not representative of the data the model will be applied to,
the model's performance on the validation set may not be indicative of its
performance on unseen data.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>In
conclusion, the fact that a TensorFlow model's training accuracy is around 100%
but its validation accuracy is only around 50% even after using data
augmentation techniques like Spec Augment indicates that the model is still
overfitting. Overfitting occurs when a model learns patterns in the training
data that do not generalise well to unseen data, and it can result in poor
validation set performance. In this case, additional approaches to addressing
overfitting may be required, such as using a simpler model architecture,
increasing the amount of regularisation, or collecting more diverse training
data. The hyperparameter selection and validation set composition must be
re-evaluated to ensure that they are appropriate for the task. Overall, it is
critical to ensure that the model can generalise well to unseen data in order
to achieve good validation set performance.</span></p>

<p class=MsoNormal style='text-align:justify'><b><span style='font-size:14.0pt;
line-height:107%;font-family:"Times New Roman",serif'>7 <a name="_Hlk123862974">-
Conclusion</a></span></b></p>

<p class=MsoNormal style='text-align:justify;line-height:115%'><a
name="_Hlk123863033"><b><span style='font-size:14.0pt;line-height:115%;
font-family:"Times New Roman",serif'>7.1 - Project Summary</span></b></a></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>The
goal of this project was to classify emotions in speech audio using image classification
techniques. To begin, we converted the speech audio into spectrogram images,
which are visual representations of a sound's frequency spectrum. TensorFlow
was then used to train a convolutional neural network (CNN) on a dataset of
spectrogram images, each labelled with an emotion.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>We
discovered that the model struggled to achieve high accuracy during training.
We discovered that after training and evaluating the model on a validation set,
it only achieved an accuracy of around 50%. This suggests that using
spectrogram images and image classification techniques to accurately classify
emotions in speech may be ineffective.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>One
possible explanation for the poor accuracy is that the features captured by
spectrogram images are insufficient for accurately representing emotions in
speech. Another possibility is that the CNN architecture employed in the model
is unsuitable for this task. Further research and exploration of alternative
methods and models may be required to improve the model's performance.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:115%'><a
name="_Hlk123863043"><b><span style='font-size:14.0pt;line-height:115%;
font-family:"Times New Roman",serif'>7.2 - Limitations </span></b></a></p>

<p class=MsoNormal style='text-align:justify;line-height:115%'><span
style='font-size:14.0pt;line-height:115%;font-family:"Times New Roman",serif'>There
are a few limitations regarding this completed research.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>The
dataset only includes recordings from a small number of actors (24 actors in
total, 12 male and 12 female). This limited number of actors may not be representative
of the overall population, and the model may not generalize well to other
speakers. The classification models can be sensitive to the class distribution
of the dataset, it may have affected the performance of the model.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Speech
emotion recognition is a challenging task, and it can be difficult to achieve
high accuracy using a single modality (e.g., only using spectrograms). Spectrograms
are sensitive to the type of window used to compute the short-time Fourier
transform. Different window types can emphasize different aspects of the
signal, which can affect the performance of the model.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Speech
emotion recognition is a highly contextual task, and the emotional content of a
speech signal can depend on the words being spoken, the speaker's tone of voice,
and the context in which the speech is being given. Capturing this contextual
information may be difficult using a spectrogram alone.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>A
TensorFlow image classifier is a powerful tool for building machine learning
models, but it may not be the most suitable tool for speech emotion
recognition. Image classification models can be sensitive to the quality and
resolution of the input spectrogram images. If the images are noisy or of low
quality, it can make it difficult for the model to learn useful features.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><b><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>7.3
<a name="_Hlk123863073">- Future Research Options</a></span></b></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>One
obvious research option is improving the validation accuracy of this model by
testing different other hyperparameters, optimisers, activation functions etc.
A different image classifying architecture could be employed.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Improved
data augmentation: Researchers could investigate using more advanced data
augmentation techniques to improve the generalizability of the model.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Improved
feature representation: Researchers could explore using more advanced
techniques for representing the spectrogram data, such as using
higher-dimensional representations or incorporating additional information
about the signal.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Improved
classification models: Researchers could explore using more advanced machine
learning models (e.g., deep learning models) to improve the performance of the
model.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Researchers
could investigate using multiple modalities (e.g., audio, text, and facial
expressions) to improve the performance of the model.</span></p>

<p class=MsoNormal style='text-align:justify;line-height:150%'><span
style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'>Researchers
could investigate using transfer learning techniques to leverage pre-trained
models and improve the performance of the model on the target task.</span></p>

</div>

<span style='font-size:14.0pt;line-height:150%;font-family:"Times New Roman",serif'><br
clear=all style='page-break-before:always'>
</span>

<div class=WordSection2>

<p class=MsoNormal style='line-height:150%'><b><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>References</span></b></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Abadi M et al.,
&quot;TensorFlow: A system for large-scale machine learning,&quot; in
Proceedings of the 12th USENIX Symposium on Operating Systems Design and
Implementation, 2015, pp. 265-283.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Abdelwahab, M., Busso,
C., 2018. Domain adversarial for acoustic emotion recognition.IEEE/ACM Trans.
Audio Speech Lang. Process. 26 (12), 2423-2435.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Albornoz, E.M., Milone,
D.H., Rufiner, H.L., 2011. Spoken emotion recognition using hierarchical
classifiers. Comput. Speech Lang. 25 (3), 556-570.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Anagnostopoulos, C.-N.,
Iliou, T., Giannoukos, I., 2015. Features and classifiers for emotion
recognition from speech: a survey from 2000 to 2011. Artif. Intell. Rev. 43
(2),155-177.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Asgari, M., Kiss, G., Van
Santen, J., Shafran, I., Song, X., 2014. Automatic measurementof affective
valence and arousal in speech. In: 2014 IEEE International Conference
onAcoustics, Speech and Signal Processing (ICASSP). IEEE, pp. 965-969.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Ayadi, M.E., Kamel, M.S.,
Karray, F., 2011. Survey on speech emotion recognition: features, classification
schemes, and databases. Patt. Recog. 44 (3), 572-587.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Bachorowski, J.-A., 1999.
Vocal expression and perception of emotion. Curr. Dir. Psychol.Sci. 8 (2), 53-57.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Bachu, R., Kopparthi, S.,
Adapa, B., Barkana, B., 2010. Voiced/unvoiced decision forspeech signals based
on zero-crossing rate and energy. In: Elleithy, K. (Ed.), AdvancedTechniques in
Computing Sciences and Software Engineering. Springer Netherlands,Dordrecht,
pp. 279-282.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Bahdanau, D., Cho, K.,
Bengio, Y., 2014. Neural machine translation by jointly learningto align and
translate. arXiv:1409.0473.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Basu, S., Chakraborty,
J., Bag, A., Aftabuddin, M., 2017. A review on emotion recognition using
speech. In: 2017 International Conference on Inventive Communication and
Computational Technologies (ICICCT), pp. 109-114.
doi:10.1109/ICICCT.2017.7975169.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Batliner, A., Steidl, S.,
Noth, E., 2008. Releasing a thoroughly annotated and processedspontaneous
emotional database: the fau aibo emotion corpus. In: Proc. of a
SatelliteWorkshop of LREC, 2008, p. 28.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Bitouk, D., Verma, R.,
Nenkova, A., 2010. Class-level spectral features for emotion recognition.
Speech Commun. 52 (7-8), 613-625.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Borchert, M., Dusterhoft,
A., 2005. Emotions in speech-experiments with prosody and</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>quality features in
speech for use in categorical and dimensional emotion recognition environments.
In: Natural Language Processing and Knowledge Engineering,2005. IEEE NLP-KE05.
Proceedings of 2005 IEEE International Conference on. IEEE,pp. 147-151.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Burkhardt, F., Paeschke,
A., Rolfes, M., Sendlmeier, W.F., Weiss, B., 2005. A database of German
emotional speech.. In: Interspeech. ISCA, pp. 1517-1520.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Busso, C., Deng, Z.,
Yildirim, S., Bulut, M., Lee, C.M., Kazemzadeh, A., Lee, S., Neumann, U.,
Narayanan, S., 2004. Analysis of emotion recognition using facial expressions,
speech and multimodal information. In: Proceedings of the 6th
internationalconference on Multimodal interfaces. ACM, pp. 205-211.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Busso, C., Lee, S.,
Narayanan, S., 2009. Analysis of emotionally salient aspects of fundamental frequency
for emotion detection. IEEE Trans. Audio Speech Lang. Process. 17(4), 582-596.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Caruana, R., 1997.
Multitask learning. Mach. Learn. 28 (1), 41-75.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Chen, L.S., Huang, T.S.,
Miyasato, T., Nakatsu, R., 1998. Multimodal human emotion/expression recognition.
In: Automatic Face and Gesture Recognition, 1998. Proceedings. Third IEEE
International Conference on. IEEE, pp. 366-371.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Chen, M., He, X., Yang,
J., Zhang, H., 2018. 3-D convolutional recurrent neural networkswith attention
model for speech emotion recognition. IEEE Signal Process. Lett. 25(10), 1440-1444.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Costantini, G., Iaderola,
I., Paoloni, A., Todisco, M., 2014. Emovo corpus: an italianemotional speech
database. In: International Conference on Language Resources and Evaluation
(LREC 2014). European Language Resources Association (ELRA),pp. 3501-3504.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Cowie, R., Douglas-Cowie,
E., 1996. Automatic statistical analysis of the signal and prosodic signs of
emotion in speech. In: Fourth International Conference on Spoken Language
Processing.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Cowie, R., Douglas-Cowie,
E., Tsapatsoulis, N., Votsis, G., Kollias, S., Fellenz, W., Taylor, J.G., 2001.
Emotion recognition in human-computer interaction. IEEE Signal Process. Mag. 18
(1), 32-80. doi:10.1109/79.911197.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Cowie, R., Douglas-Cowie,
E., Tsapatsoulis, N., Votsis, G., Kollias, S., Fellenz, W., Taylor, J.G., 2001.
Emotion recognition in human-computer interaction. IEEE Signal Process. Mag. 18
(1), 32-80. doi:10.1109/79.911197.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Deng, J., Xu, X., Zhang,
Z., Frhholz, S., Schuller, B., 2017. Semi supervised autoencoders for speech
emotion recognition. IEEE/ACM Trans. Audio Speech Lang. Process. 26</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Deng, J., Zhang, Z.,
Eyben, F., Schuller, B., 2014. Autoencoder-based unsupervised domain adaptation
for speech emotion recognition. IEEE Signal Process. Lett. 21 (9),1068-1072.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Ekman, P., 1971.
Universals and cultural differences in facial expressions of emotion..Nebraska
symposium on motivation. University of Nebraska Press.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Ekman, P., Friesen, W.V.,
Ellsworth, P., 2013. Emotion in the Human Face: Guidelines for Research and an
Integration of Findings. Elsevier.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Ekman, P., Oster, H.,
1979. Facial expressions of emotion. Ann. Rev. Psychol. 30 (1), 527-554.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Engberg, I.S., Hansen,
A.V., Andersen, O., Dalsgaard, P., 1997. Design, recording and verification of
a Danish emotional speech database. In: Fifth European Conference on Speech
Communication and Technology.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Eskimez, S.E., Duan, Z.,
Heinzelman, W., 2018. Unsupervised learning approach to featureanalysis for
automatic speech emotion recognition. In: 2018 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP). IEEE, pp. 5099-5103.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Eyben, F., Wllmer, M.,
Graves, A., Schuller, B., Douglas-Cowie, E., Cowie, R., 2010. On-line emotion
recognition in a 3-d activation-valence-time continuum using acoustic and
linguistic cues. J. Multimodal User Interf. 3 (1-2), 7-19.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Frick, R.W., 1985.
Communicating emotion: the role of prosodic features.. Psychol. Bull. 97 (3),
412.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Gideon, J., Khorram, S.,
Aldeneh, Z., Dimitriadis, D., Provost, E.M., 2017. Progressive neural networks
for transfer learning in emotion recognition. Proc. Interspeech 2017, 1098-1102.
doi:10.21437/Interspeech.2017-1637.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Gobl, C., Chasaide, A.N.,
2003. The role of voice quality in communicating emotion, mood and attitude.
Speech Commun. 40 (1-2), 189-212.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Gupta, P., Rajput, N.
(2007) Two-stream emotion recognition for call center monitoring. Proc.
Interspeech 2007, 2241-2244, doi: 10.21437/Interspeech.2007-609 (1), 31-43</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Huang, C.-W., Narayanan,
S.S., 2017. Deep convolutional recurrent neural network with attention
mechanism for robust speech emotion recognition. In: 2017 IEEE International
Conference on Multimedia and Expo (ICME). IEEE, pp. 583-588. Iemocap database.
2019. <a href="https://sail.usc.edu/iemocap/">https://sail.usc.edu/iemocap/</a>.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Jain A. K. et al.,
&quot;A review on deep learning techniques applied to image
classification,&quot; Expert Systems with Applications, vol. 116, pp. 1-14,
2019.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Kaiser, J.F., 1990. On a
simple algorithm to calculate the energy of a signal. In: Acoustics, Speech,
and Signal Processing, 1990. ICASSP-90., 1990 International Conference on. IEEE,
pp. 381-384.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Lugger, M., Yang, B.,
2008. Psychological motivated multi-stage emotion classification exploiting
voice quality features. Speech Recognition. InTech.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Mangalam, K., Guha, T.,
2018. Learning spontaneity to improve emotion recognition in speech. Proc.
Interspeech 2018, 946-950. doi:10.21437/Interspeech.2018-1872.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Mao, Q., Dong, M., Huang,
Z., Zhan, Y., 2014. Learning salient features for speech emotion recognition
using convolutional neural networks. IEEE Trans. Multimed. 16 (8), 2203-2213.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Murray, I.R., Arnott,
J.L., 1993. Toward the simulation of emotion in synthetic speech: a review of
the literature on human vocal emotion. J. Acoust. Soc. Am. 93 (2), 1097-1108.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Nakatsu, R., Nicholson,
J., Tosa, N., 1999. Emotion recognition and its application to computer agents
with spontaneous interactive capabilities. In: Proceedings of the seventh ACM
international conference on Multimedia (Part 1). ACM, pp. 343- 351.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Neiberg, D., Elenius, K.,
Laskowski, K., 2006. Emotion recognition in spontaneous speech using gmms. In:
Ninth International Conference on Spoken Language Processing.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Neumann, M., et al.,
2018. Cross-lingual and multilingual speech emotion recognition on english and
french. In: 2018 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP). IEEE, pp. 5769-5773.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Nicholson, J., Takahashi,
K., Nakatsu, R., 2000. Emotion recognition in speech using neural networks.
Neur. Comput. Appl. 9 (4), 290-296.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Nwe, T.L., Foo, S.W., De
Silva, L.C., 2003. Detection of stress and emotion in speech using traditional
and fft based log energy features. In: Information, Communications and Signal
Processing, 2003 and Fourth Pacific Rim Conference on Multimedia. Proceedings
of the 2003 Joint Conference of the Fourth International Conference on, 3.
IEEE, pp. 1619-1623.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Nwe, T.L., Foo, S.W., De
Silva, L.C., 2003. Speech emotion recognition using hidden markov models.
Speech Commun. 41 (4), 603-623. 3. IEEE, pp. 149-152. </span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Szwoch, M., Szwoch, W.,
2015. Emotion recognition for affect aware video games. In:Chora&#347;, R.S.
(Ed.), Image Processing &amp; Communications Challenges 6. Springer International
Publishing, Cham, pp. 227-236.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Teager, H., Teager, S.,
1990. Evidence for nonlinear sound production mechanisms in the vocal tract.
In: Speech production and speech modelling. Springer, pp. 241-261.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Tian, L., Moore, J., Lai,
C., 2016. Recognizing emotions in spoken dialogue with hierarchically fused
acoustic and lexical features. In: 2016 IEEE Spoken Language Technology Workshop
(SLT). IEEE, pp. 565-572.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Trigeorgis, G., Ringeval,
F., Brueckner, R., Marchi, E., Nicolaou, M.A., Schuller, B., Zafeiriou, S.,
2016. Adieu features? end-to-end speech emotion recognition using a deep
convolutional recurrent network. In: Acoustics, Speech and Signal Processing (ICASSP),
2016 IEEE International Conference on. IEEE, pp. 5200-5204.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Truong, K.P., Van Leeuwen,
D.A., De Jong, F.M., 2012. Speech-based recognition Watson, D., Clark, L.A.,
Tellegen, A., 1988. Development and validation of brief measures of positive
and negative affect: the panas scales.. J. Personal. Soc. Psychol. 54 (6), 1063.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Wllmer, M., Eyben, F.,
Reiter, S., Schuller, B., Cox, C., Douglas-Cowie, E., Cowie, R., 2008.
Abandoning emotion classes-towards continuous emotion recognition with modelling
of long-range dependencies. In: Proc. 9th Interspeech 2008 incorp. 12<sup>th</sup>
Australasian Int. Conf. on Speech Science and Technology SST 2008, Brisbane,
Australia, pp. 597-600.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Wollmer, M., Schuller,
B., Eyben, F., Rigoll, G., 2010. Combining long short-term memory and dynamic
bayesian networks for incremental emotion-sensitive artificial listening. IEEE
J. Select. Top. Signal Process. 4 (5), 867-881.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>Zhang J and . Chen, S &quot;Deep
learning for image classification: A review,&quot; Neurocomputing, vol. 273,
pp. 3-19, 2018.</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal style='line-height:150%'><span style='font-size:14.0pt;
line-height:150%;font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal><span style='font-size:14.0pt;line-height:107%;font-family:
"Times New Roman",serif'>&nbsp;</span></p>

</div>

</body>

</html>
